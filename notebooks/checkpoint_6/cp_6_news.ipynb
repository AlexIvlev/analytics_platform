{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47cafb0d-d025-4eb3-a987-ae117bc4c125",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Глубинное обучение\n",
    "\n",
    "Классические модели машинного обучения показали ограниченную эффективность при решении задачи классификации направления ценового движения на основе финансовых текстов. Одной из причин являются ограничения в обработке контекста и последовательностей слов, которые играют ключевую роль в анализе текстовой информации.\n",
    "\n",
    "Глубинное обучение позволяет учитывать сложные языковые зависимости и взаимодействия признаков. В этой части исследования реализуем три модели нейронных сетей с использованием PyTorch.\n",
    "Для ускорения обучения и сохранения интерпретируемости выбраны лёгкие архитектуры: TextCNN, biGRU и FastText-style. Для всех моделей используем те же признаки и целевую переменную, что и ранее, но на увеличенном датасете (50 000 строк). Метрики оценки - accuracy и F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1da3d4-0382-42ce-86a2-43cf45f91c81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Device Name: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5728a78d-49c9-4d8a-845b-9dddb0cab3d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    25000\n",
      "0    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Подготовка датасета из 50 тыс. строк с равномерным распределением целевой переменной\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.utils import resample\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "files = sorted(glob.glob('000[0-6].parquet'))\n",
    "df = pd.concat([pd.read_parquet(f, engine='pyarrow') for f in files], ignore_index=True)\n",
    "\n",
    "df = df.iloc[:200_000]\n",
    "\n",
    "df['price_24h_change_percent'] = ((df['weighted_avg_24_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "df['target'] = (df['price_24h_change_percent'] > 0).astype(int)\n",
    "\n",
    "df_0 = df[df['target'] == 0]\n",
    "df_1 = df[df['target'] == 1]\n",
    "\n",
    "df_0_sample = resample(df_0, replace=False, n_samples=25_000, random_state=RANDOM_SEED)\n",
    "df_1_sample = resample(df_1, replace=False, n_samples=25_000, random_state=RANDOM_SEED)\n",
    "\n",
    "df = pd.concat([df_0_sample, df_1_sample]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "511b3300-080f-4380-889a-97ffd295906d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Symbol                    50000 non-null  object \n",
      " 1   Security                  50000 non-null  object \n",
      " 2   Sector                    49059 non-null  object \n",
      " 3   Industry                  49059 non-null  object \n",
      " 4   URL                       50000 non-null  object \n",
      " 5   Date                      50000 non-null  object \n",
      " 6   RelatedStocksList         36910 non-null  object \n",
      " 7   Article                   50000 non-null  object \n",
      " 8   Title                     49915 non-null  object \n",
      " 9   articleType               50000 non-null  object \n",
      " 10  Publication               49958 non-null  object \n",
      " 11  Author                    34490 non-null  object \n",
      " 12  weighted_avg_-96_hrs      50000 non-null  float64\n",
      " 13  weighted_avg_-48_hrs      50000 non-null  float64\n",
      " 14  weighted_avg_-24_hrs      50000 non-null  float64\n",
      " 15  weighted_avg_0_hrs        50000 non-null  float64\n",
      " 16  weighted_avg_0.25_hrs     50000 non-null  float64\n",
      " 17  weighted_avg_0.50_hrs     50000 non-null  float64\n",
      " 18  weighted_avg_1_hrs        50000 non-null  float64\n",
      " 19  weighted_avg_2_hrs        50000 non-null  float64\n",
      " 20  weighted_avg_4_hrs        50000 non-null  float64\n",
      " 21  weighted_avg_6_hrs        50000 non-null  float64\n",
      " 22  weighted_avg_8_hrs        50000 non-null  float64\n",
      " 23  weighted_avg_12_hrs       50000 non-null  float64\n",
      " 24  weighted_avg_24_hrs       50000 non-null  float64\n",
      " 25  weighted_avg_48_hrs       50000 non-null  float64\n",
      " 26  weighted_avg_72_hrs       50000 non-null  float64\n",
      " 27  weighted_avg_96_hrs       50000 non-null  float64\n",
      " 28  weighted_avg_360_hrs      50000 non-null  float64\n",
      " 29  weighted_avg_720_hrs      50000 non-null  float64\n",
      " 30  price_24h_change_percent  48695 non-null  float64\n",
      " 31  target                    50000 non-null  int32  \n",
      " 32  text                      50000 non-null  object \n",
      "dtypes: float64(19), int32(1), object(13)\n",
      "memory usage: 12.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>RelatedStocksList</th>\n",
       "      <th>Article</th>\n",
       "      <th>Title</th>\n",
       "      <th>articleType</th>\n",
       "      <th>...</th>\n",
       "      <th>weighted_avg_12_hrs</th>\n",
       "      <th>weighted_avg_24_hrs</th>\n",
       "      <th>weighted_avg_48_hrs</th>\n",
       "      <th>weighted_avg_72_hrs</th>\n",
       "      <th>weighted_avg_96_hrs</th>\n",
       "      <th>weighted_avg_360_hrs</th>\n",
       "      <th>weighted_avg_720_hrs</th>\n",
       "      <th>price_24h_change_percent</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAN</td>\n",
       "      <td>Dana Incorporated</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Auto Parts:O.E.M.</td>\n",
       "      <td>https://www.nasdaq.com/articles/dan-crosses-be...</td>\n",
       "      <td>Mar 14, 2019 11:48 AM ET</td>\n",
       "      <td>Markets</td>\n",
       "      <td>In trading on Thursday, shares of Dana Inc (Sy...</td>\n",
       "      <td>DAN Crosses Below Key Moving Average Level</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0960</td>\n",
       "      <td>18.1922</td>\n",
       "      <td>18.2424</td>\n",
       "      <td>18.2528</td>\n",
       "      <td>18.4231</td>\n",
       "      <td>17.6645</td>\n",
       "      <td>20.5289</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "      <td>DAN Crosses Below Key Moving Average Level In ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FCF</td>\n",
       "      <td>First Commonwealth Financial Corporation</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Major Banks</td>\n",
       "      <td>https://www.nasdaq.com/articles/validea-peter-...</td>\n",
       "      <td>Feb 22, 2024 05:34 AM ET</td>\n",
       "      <td>Markets|ROCK|VC</td>\n",
       "      <td>The following are today's upgrades for Validea...</td>\n",
       "      <td>Validea Peter Lynch Strategy Daily Upgrade Rep...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1567</td>\n",
       "      <td>13.1727</td>\n",
       "      <td>13.2255</td>\n",
       "      <td>13.1243</td>\n",
       "      <td>13.1187</td>\n",
       "      <td>13.6013</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>Validea Peter Lynch Strategy Daily Upgrade Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGC</td>\n",
       "      <td>Canopy Growth Corporation</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Medicinal Chemicals and Botanical Products</td>\n",
       "      <td>https://www.nasdaq.com/articles/cronos-group-3...</td>\n",
       "      <td>Apr 08, 2019 05:23 PM ET</td>\n",
       "      <td>CRON|Markets|TLRY|ACB|MO</td>\n",
       "      <td>**Cronos Group** (NASDAQ:), a top Canadian can...</td>\n",
       "      <td>Cronos Group: 3 Pros, 3 Cons for Buying CRON S...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>42.6443</td>\n",
       "      <td>42.4592</td>\n",
       "      <td>41.9311</td>\n",
       "      <td>42.0994</td>\n",
       "      <td>41.4073</td>\n",
       "      <td>48.2890</td>\n",
       "      <td>47.4887</td>\n",
       "      <td>-2.79</td>\n",
       "      <td>0</td>\n",
       "      <td>Cronos Group: 3 Pros, 3 Cons for Buying CRON S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECPG</td>\n",
       "      <td>Encore Capital Group, Inc.</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Finance Companies</td>\n",
       "      <td>https://www.nasdaq.com/articles/encore-capital...</td>\n",
       "      <td>Jan 15, 2022 12:33 AM ET</td>\n",
       "      <td>Stocks</td>\n",
       "      <td>Encore Capital Group, Inc. ([ECPG](https://kwh...</td>\n",
       "      <td>Encore Capital Group, Inc. Shares Climb 3.9% P...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0376</td>\n",
       "      <td>67.7071</td>\n",
       "      <td>67.0765</td>\n",
       "      <td>66.8197</td>\n",
       "      <td>67.1355</td>\n",
       "      <td>65.3444</td>\n",
       "      <td>70.3295</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>Encore Capital Group, Inc. Shares Climb 3.9% P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARCT</td>\n",
       "      <td>Arcturus Therapeutics Holdings Inc.</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology: Pharmaceutical Preparations</td>\n",
       "      <td>https://www.nasdaq.com/press-release/arcturus-...</td>\n",
       "      <td>Apr 13, 2020 08:01 AM ET</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical Plan Includes Healthy Volunteers in N...</td>\n",
       "      <td>Arcturus Therapeutics Announces Allowance of I...</td>\n",
       "      <td>Press Release</td>\n",
       "      <td>...</td>\n",
       "      <td>16.5429</td>\n",
       "      <td>16.8677</td>\n",
       "      <td>18.3530</td>\n",
       "      <td>17.1136</td>\n",
       "      <td>17.1753</td>\n",
       "      <td>32.4890</td>\n",
       "      <td>46.7779</td>\n",
       "      <td>9.15</td>\n",
       "      <td>1</td>\n",
       "      <td>Arcturus Therapeutics Announces Allowance of I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                                  Security                  Sector  \\\n",
       "0    DAN                         Dana Incorporated  Consumer Discretionary   \n",
       "1    FCF  First Commonwealth Financial Corporation                 Finance   \n",
       "2    CGC                 Canopy Growth Corporation             Health Care   \n",
       "3   ECPG                Encore Capital Group, Inc.                 Finance   \n",
       "4   ARCT       Arcturus Therapeutics Holdings Inc.             Health Care   \n",
       "\n",
       "                                       Industry  \\\n",
       "0                             Auto Parts:O.E.M.   \n",
       "1                                   Major Banks   \n",
       "2   Medicinal Chemicals and Botanical Products    \n",
       "3                             Finance Companies   \n",
       "4    Biotechnology: Pharmaceutical Preparations   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.nasdaq.com/articles/dan-crosses-be...   \n",
       "1  https://www.nasdaq.com/articles/validea-peter-...   \n",
       "2  https://www.nasdaq.com/articles/cronos-group-3...   \n",
       "3  https://www.nasdaq.com/articles/encore-capital...   \n",
       "4  https://www.nasdaq.com/press-release/arcturus-...   \n",
       "\n",
       "                       Date         RelatedStocksList  \\\n",
       "0  Mar 14, 2019 11:48 AM ET                   Markets   \n",
       "1  Feb 22, 2024 05:34 AM ET           Markets|ROCK|VC   \n",
       "2  Apr 08, 2019 05:23 PM ET  CRON|Markets|TLRY|ACB|MO   \n",
       "3  Jan 15, 2022 12:33 AM ET                    Stocks   \n",
       "4  Apr 13, 2020 08:01 AM ET                      None   \n",
       "\n",
       "                                             Article  \\\n",
       "0  In trading on Thursday, shares of Dana Inc (Sy...   \n",
       "1  The following are today's upgrades for Validea...   \n",
       "2  **Cronos Group** (NASDAQ:), a top Canadian can...   \n",
       "3  Encore Capital Group, Inc. ([ECPG](https://kwh...   \n",
       "4  Clinical Plan Includes Healthy Volunteers in N...   \n",
       "\n",
       "                                               Title    articleType  ...  \\\n",
       "0         DAN Crosses Below Key Moving Average Level           News  ...   \n",
       "1  Validea Peter Lynch Strategy Daily Upgrade Rep...           News  ...   \n",
       "2  Cronos Group: 3 Pros, 3 Cons for Buying CRON S...           News  ...   \n",
       "3  Encore Capital Group, Inc. Shares Climb 3.9% P...           News  ...   \n",
       "4  Arcturus Therapeutics Announces Allowance of I...  Press Release  ...   \n",
       "\n",
       "  weighted_avg_12_hrs weighted_avg_24_hrs  weighted_avg_48_hrs  \\\n",
       "0             18.0960             18.1922              18.2424   \n",
       "1             13.1567             13.1727              13.2255   \n",
       "2             42.6443             42.4592              41.9311   \n",
       "3             68.0376             67.7071              67.0765   \n",
       "4             16.5429             16.8677              18.3530   \n",
       "\n",
       "   weighted_avg_72_hrs  weighted_avg_96_hrs  weighted_avg_360_hrs  \\\n",
       "0              18.2528              18.4231               17.6645   \n",
       "1              13.1243              13.1187               13.6013   \n",
       "2              42.0994              41.4073               48.2890   \n",
       "3              66.8197              67.1355               65.3444   \n",
       "4              17.1136              17.1753               32.4890   \n",
       "\n",
       "   weighted_avg_720_hrs  price_24h_change_percent  target  \\\n",
       "0               20.5289                      0.66       1   \n",
       "1               13.5000                     -0.27       0   \n",
       "2               47.4887                     -2.79       0   \n",
       "3               70.3295                     -0.49       0   \n",
       "4               46.7779                      9.15       1   \n",
       "\n",
       "                                                text  \n",
       "0  DAN Crosses Below Key Moving Average Level In ...  \n",
       "1  Validea Peter Lynch Strategy Daily Upgrade Rep...  \n",
       "2  Cronos Group: 3 Pros, 3 Cons for Buying CRON S...  \n",
       "3  Encore Capital Group, Inc. Shares Climb 3.9% P...  \n",
       "4  Arcturus Therapeutics Announces Allowance of I...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предобработка - убираем пустые значения, добавляем поле text\n",
    "df = df.dropna(subset=['Article', 'weighted_avg_12_hrs'])\n",
    "df['text'] = (df['Title'] + ' ' + df['Article']).fillna('')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3d9afd5-f3e1-4cf0-b95a-3e33bb4cf9bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d4f472dacf4a98968956b7094794c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentiment Analysis:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Делаем сентимент-анализ с VADER, добавляем в датасет\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str):\n",
    "        return analyzer.polarity_scores(text)['compound']\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Параллельная обработка\n",
    "sentiments = Parallel(n_jobs=-1)(\n",
    "    delayed(get_sentiment)(text) for text in tqdm(df['text'], desc=\"Sentiment Analysis\")\n",
    ")\n",
    "df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "599b707b-e8ac-44da-bc6d-45303cb54dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fba7974-e257-4dbd-8d51-e23f982c7d61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Предобработка\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "# Считаем частоты токенов без создания большого списка\n",
    "token_counter = Counter()\n",
    "for text in df['text']:\n",
    "    token_counter.update(tokenize(text))\n",
    "\n",
    "# Строим словарь на основе самых частотных токенов\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "vocab.update({word: i+2 for i, (word, _) in enumerate(token_counter.most_common(20_000))})\n",
    "\n",
    "def encode(text, max_len=200):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    padded = ids[:max_len] + [vocab['<PAD>']] * (max_len - len(ids))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf2643f8-2d29-402e-9aa6-15b27804e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, sentiments, labels):\n",
    "        self.texts = texts\n",
    "        self.sentiments = sentiments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            'sentiment': torch.tensor([self.sentiments[idx]], dtype=torch.float),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Кодирование текста\n",
    "encoded_texts = [encode(text) for text in df['text']]\n",
    "sentiments = df['sentiment'].tolist()\n",
    "labels = df['target'].tolist()\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train, X_val, s_train, s_val, y_train, y_val = train_test_split(\n",
    "    encoded_texts, sentiments, labels, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "train_dataset = NewsDataset(X_train, s_train, y_train)\n",
    "val_dataset = NewsDataset(X_val, s_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c7a99c1-3204-4b98-b9ea-e2a76838e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes=1):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=100, kernel_size=k) for k in [3, 4, 5]\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(100 * 3 + 1, num_classes)  # +1 для sentiment\n",
    "\n",
    "    def forward(self, x_text, x_sentiment):\n",
    "        x = self.embedding(x_text).transpose(1, 2)  # (B, E, L)\n",
    "        x = [torch.relu(conv(x)).max(dim=2)[0] for conv in self.convs]  # max-pooling\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = torch.cat([x, x_sentiment], dim=1)  # добавляем sentiment как признак\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd9135a9-ec2c-4475-98de-aada2d186171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7212\n",
      "Accuracy: 0.5058 | F1 Score: 0.0214\n",
      "Epoch 2 | Loss: 0.6983\n",
      "Accuracy: 0.5099 | F1 Score: 0.6636\n",
      "Epoch 3 | Loss: 0.6978\n",
      "Accuracy: 0.5246 | F1 Score: 0.6247\n",
      "Epoch 4 | Loss: 0.6914\n",
      "Accuracy: 0.5214 | F1 Score: 0.6466\n",
      "Epoch 5 | Loss: 0.6888\n",
      "Accuracy: 0.5255 | F1 Score: 0.6553\n",
      "Epoch 6 | Loss: 0.6785\n",
      "Accuracy: 0.5304 | F1 Score: 0.6336\n",
      "Epoch 7 | Loss: 0.6607\n",
      "Accuracy: 0.5357 | F1 Score: 0.4592\n",
      "Epoch 8 | Loss: 0.6367\n",
      "Accuracy: 0.5454 | F1 Score: 0.5725\n",
      "Epoch 9 | Loss: 0.5998\n",
      "Accuracy: 0.5385 | F1 Score: 0.6223\n",
      "Epoch 10 | Loss: 0.5594\n",
      "Accuracy: 0.5425 | F1 Score: 0.5123\n",
      "Epoch 11 | Loss: 0.5112\n",
      "Accuracy: 0.5463 | F1 Score: 0.5724\n",
      "Epoch 12 | Loss: 0.4642\n",
      "Accuracy: 0.5422 | F1 Score: 0.5206\n",
      "Epoch 13 | Loss: 0.4187\n",
      "Accuracy: 0.5394 | F1 Score: 0.5023\n",
      "Epoch 14 | Loss: 0.3848\n",
      "Accuracy: 0.5382 | F1 Score: 0.4993\n",
      "Epoch 15 | Loss: 0.3485\n",
      "Accuracy: 0.5392 | F1 Score: 0.5864\n",
      "Epoch 16 | Loss: 0.3204\n",
      "Accuracy: 0.5426 | F1 Score: 0.5562\n",
      "Epoch 17 | Loss: 0.2967\n",
      "Accuracy: 0.5390 | F1 Score: 0.5419\n",
      "Epoch 18 | Loss: 0.2760\n",
      "Accuracy: 0.5394 | F1 Score: 0.5552\n",
      "Epoch 19 | Loss: 0.2532\n",
      "Accuracy: 0.5380 | F1 Score: 0.5457\n",
      "Epoch 20 | Loss: 0.2418\n",
      "Accuracy: 0.5403 | F1 Score: 0.5533\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextCNN(vocab_size=len(vocab), embed_dim=100).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        sentiments = batch['sentiment'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, sentiments)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            sentiments = batch['sentiment'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(texts, sentiments)\n",
    "            preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(truths, preds)\n",
    "    f1 = f1_score(truths, preds)\n",
    "    print(f\"Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bdb9bd0-b705-4d43-9e27-5c2dc3dc2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU\n",
    "class BiGRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        super(BiGRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + 1, 1)  # +1 для sentiment\n",
    "\n",
    "    def forward(self, x_text, x_sentiment):\n",
    "        embedded = self.embedding(x_text)  # (B, L, E)\n",
    "        _, h_n = self.gru(embedded)        # h_n: (2, B, H)\n",
    "        h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)  # (B, 2H)\n",
    "        x = torch.cat([h_n, x_sentiment], dim=1)    # добавим sentiment\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b1a3745-f776-4272-8408-d52ce0e950b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6951\n",
      "Accuracy: 0.5164 | F1 Score: 0.5590\n",
      "Epoch 2 | Loss: 0.6855\n",
      "Accuracy: 0.5312 | F1 Score: 0.5407\n",
      "Epoch 3 | Loss: 0.6690\n",
      "Accuracy: 0.5446 | F1 Score: 0.5780\n",
      "Epoch 4 | Loss: 0.6388\n",
      "Accuracy: 0.5375 | F1 Score: 0.5089\n",
      "Epoch 5 | Loss: 0.5938\n",
      "Accuracy: 0.5394 | F1 Score: 0.5247\n",
      "Epoch 6 | Loss: 0.5324\n",
      "Accuracy: 0.5359 | F1 Score: 0.5633\n",
      "Epoch 7 | Loss: 0.4648\n",
      "Accuracy: 0.5357 | F1 Score: 0.5287\n",
      "Epoch 8 | Loss: 0.3948\n",
      "Accuracy: 0.5329 | F1 Score: 0.5454\n",
      "Epoch 9 | Loss: 0.3266\n",
      "Accuracy: 0.5350 | F1 Score: 0.5357\n",
      "Epoch 10 | Loss: 0.2691\n",
      "Accuracy: 0.5276 | F1 Score: 0.5202\n",
      "Epoch 11 | Loss: 0.2243\n",
      "Accuracy: 0.5297 | F1 Score: 0.5342\n",
      "Epoch 12 | Loss: 0.1938\n",
      "Accuracy: 0.5281 | F1 Score: 0.5187\n",
      "Epoch 13 | Loss: 0.1687\n",
      "Accuracy: 0.5275 | F1 Score: 0.5299\n",
      "Epoch 14 | Loss: 0.1514\n",
      "Accuracy: 0.5244 | F1 Score: 0.5351\n",
      "Epoch 15 | Loss: 0.1339\n",
      "Accuracy: 0.5274 | F1 Score: 0.5273\n",
      "Epoch 16 | Loss: 0.1641\n",
      "Accuracy: 0.5306 | F1 Score: 0.5221\n",
      "Epoch 17 | Loss: 0.1361\n",
      "Accuracy: 0.5229 | F1 Score: 0.5258\n",
      "Epoch 18 | Loss: 0.1179\n",
      "Accuracy: 0.5256 | F1 Score: 0.5081\n",
      "Epoch 19 | Loss: 0.1013\n",
      "Accuracy: 0.5300 | F1 Score: 0.5353\n",
      "Epoch 20 | Loss: 0.0951\n",
      "Accuracy: 0.5286 | F1 Score: 0.5365\n"
     ]
    }
   ],
   "source": [
    "model = BiGRUClassifier(vocab_size=len(vocab), embed_dim=100, hidden_dim=64).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        sentiments = batch['sentiment'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, sentiments)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            sentiments = batch['sentiment'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(texts, sentiments)\n",
    "            preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(truths, preds)\n",
    "    f1 = f1_score(truths, preds)\n",
    "    print(f\"Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe72e7eb-5862-4d09-8b0f-1f0cd8a21c04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FastText-style\n",
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.fc = nn.Linear(embed_dim + 1, 1)  # +1 для sentiment\n",
    "\n",
    "    def forward(self, x_text, x_sentiment):\n",
    "        embedded = self.embedding(x_text)  # (B, L, E)\n",
    "        mean_emb = embedded.mean(dim=1)    # усреднение по длине текста → (B, E)\n",
    "        x = torch.cat([mean_emb, x_sentiment], dim=1)  # добавим sentiment\n",
    "        return torch.sigmoid(self.fc(x)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f62c9e02-e57e-43d0-a7e4-ada2da2d32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6922\n",
      "Accuracy: 0.5252 | F1 Score: 0.4430\n",
      "Epoch 2 | Loss: 0.6870\n",
      "Accuracy: 0.5360 | F1 Score: 0.5144\n",
      "Epoch 3 | Loss: 0.6784\n",
      "Accuracy: 0.5337 | F1 Score: 0.5476\n",
      "Epoch 4 | Loss: 0.6654\n",
      "Accuracy: 0.5379 | F1 Score: 0.4915\n",
      "Epoch 5 | Loss: 0.6498\n",
      "Accuracy: 0.5372 | F1 Score: 0.5815\n",
      "Epoch 6 | Loss: 0.6334\n",
      "Accuracy: 0.5422 | F1 Score: 0.5573\n",
      "Epoch 7 | Loss: 0.6178\n",
      "Accuracy: 0.5373 | F1 Score: 0.5485\n",
      "Epoch 8 | Loss: 0.6025\n",
      "Accuracy: 0.5349 | F1 Score: 0.5589\n",
      "Epoch 9 | Loss: 0.5892\n",
      "Accuracy: 0.5368 | F1 Score: 0.5136\n",
      "Epoch 10 | Loss: 0.5764\n",
      "Accuracy: 0.5386 | F1 Score: 0.5414\n",
      "Epoch 11 | Loss: 0.5651\n",
      "Accuracy: 0.5381 | F1 Score: 0.5463\n",
      "Epoch 12 | Loss: 0.5550\n",
      "Accuracy: 0.5372 | F1 Score: 0.5381\n",
      "Epoch 13 | Loss: 0.5456\n",
      "Accuracy: 0.5353 | F1 Score: 0.5480\n",
      "Epoch 14 | Loss: 0.5368\n",
      "Accuracy: 0.5326 | F1 Score: 0.5538\n",
      "Epoch 15 | Loss: 0.5287\n",
      "Accuracy: 0.5373 | F1 Score: 0.5364\n",
      "Epoch 16 | Loss: 0.5212\n",
      "Accuracy: 0.5316 | F1 Score: 0.5531\n",
      "Epoch 17 | Loss: 0.5142\n",
      "Accuracy: 0.5360 | F1 Score: 0.5240\n",
      "Epoch 18 | Loss: 0.5076\n",
      "Accuracy: 0.5330 | F1 Score: 0.5349\n",
      "Epoch 19 | Loss: 0.5021\n",
      "Accuracy: 0.5356 | F1 Score: 0.5360\n",
      "Epoch 20 | Loss: 0.4960\n",
      "Accuracy: 0.5347 | F1 Score: 0.5484\n"
     ]
    }
   ],
   "source": [
    "model = FastTextClassifier(vocab_size=len(vocab), embed_dim=100).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        sentiments = batch['sentiment'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, sentiments)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            sentiments = batch['sentiment'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(texts, sentiments)\n",
    "            preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(truths, preds)\n",
    "    f1 = f1_score(truths, preds)\n",
    "    print(f\"Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f041f39-9918-4ed6-82ac-ae09418564dc",
   "metadata": {},
   "source": [
    "Усложним подход - добавим предобученные эмбеддинги из GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c12be961-0c8a-4df7-9443-510d9b9f950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'glove.6B.100d.txt'\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_embeddings = {}\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Создаём матрицу эмбеддингов под наш словарь\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# Преобразуем в тензор и фиксируем <PAD> на ноль\n",
    "embedding_matrix[0] = 0.0\n",
    "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# Заменяем обычный nn.Embedding на предобученный\n",
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False, padding_idx=0)\n",
    "        self.fc = nn.Linear(embed_dim + 1, 1)\n",
    "\n",
    "    def forward(self, x_text, x_sentiment):\n",
    "        embedded = self.embedding(x_text)  # (B, L, E)\n",
    "        mean_emb = embedded.mean(dim=1)    # (B, E)\n",
    "        x = torch.cat([mean_emb, x_sentiment], dim=1)\n",
    "        return torch.sigmoid(self.fc(x)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b116e78b-ffda-4cec-97d2-82e9e1f997f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6925\n",
      "Accuracy: 0.5312 | F1 Score: 0.4922\n",
      "Epoch 2 | Loss: 0.6869\n",
      "Accuracy: 0.5307 | F1 Score: 0.6059\n",
      "Epoch 3 | Loss: 0.6772\n",
      "Accuracy: 0.5339 | F1 Score: 0.5702\n",
      "Epoch 4 | Loss: 0.6630\n",
      "Accuracy: 0.5402 | F1 Score: 0.5673\n",
      "Epoch 5 | Loss: 0.6458\n",
      "Accuracy: 0.5376 | F1 Score: 0.5896\n",
      "Epoch 6 | Loss: 0.6287\n",
      "Accuracy: 0.5412 | F1 Score: 0.5748\n",
      "Epoch 7 | Loss: 0.6128\n",
      "Accuracy: 0.5416 | F1 Score: 0.5523\n",
      "Epoch 8 | Loss: 0.5980\n",
      "Accuracy: 0.5384 | F1 Score: 0.5504\n",
      "Epoch 9 | Loss: 0.5846\n",
      "Accuracy: 0.5424 | F1 Score: 0.5441\n",
      "Epoch 10 | Loss: 0.5721\n",
      "Accuracy: 0.5373 | F1 Score: 0.5489\n",
      "Epoch 11 | Loss: 0.5609\n",
      "Accuracy: 0.5373 | F1 Score: 0.5395\n",
      "Epoch 12 | Loss: 0.5511\n",
      "Accuracy: 0.5389 | F1 Score: 0.5189\n",
      "Epoch 13 | Loss: 0.5419\n",
      "Accuracy: 0.5346 | F1 Score: 0.5370\n",
      "Epoch 14 | Loss: 0.5331\n",
      "Accuracy: 0.5368 | F1 Score: 0.5154\n",
      "Epoch 15 | Loss: 0.5259\n",
      "Accuracy: 0.5373 | F1 Score: 0.5417\n",
      "Epoch 16 | Loss: 0.5180\n",
      "Accuracy: 0.5375 | F1 Score: 0.5469\n",
      "Epoch 17 | Loss: 0.5116\n",
      "Accuracy: 0.5311 | F1 Score: 0.5594\n",
      "Epoch 18 | Loss: 0.5055\n",
      "Accuracy: 0.5328 | F1 Score: 0.5650\n",
      "Epoch 19 | Loss: 0.4996\n",
      "Accuracy: 0.5353 | F1 Score: 0.5325\n",
      "Epoch 20 | Loss: 0.4944\n",
      "Accuracy: 0.5335 | F1 Score: 0.5331\n"
     ]
    }
   ],
   "source": [
    "model = FastTextClassifier(vocab_size=len(vocab), embed_dim=100).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        sentiments = batch['sentiment'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, sentiments)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            sentiments = batch['sentiment'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(texts, sentiments)\n",
    "            preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            truths.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(truths, preds)\n",
    "    f1 = f1_score(truths, preds)\n",
    "    print(f\"Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac71cb63-fe79-409e-8ab8-6fc95e009cfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Вывод по применению методов глубинного обучения\n",
    "\n",
    "Цель данного этапа - оценить применимость простых моделей глубинного обучения для решения задачи классификации направления изменения цены акции на основе финансовых текстов. Мотивацией послужили ограниченные результаты, полученные с помощью классических алгоритмов машинного обучения, не способных эффективно учитывать контекст и последовательности слов, критически важные в анализе текстовой информации.\n",
    "\n",
    "В рамках эксперимента были реализованы три лёгкие архитектуры нейронных сетей с использованием библиотеки PyTorch:  \n",
    "- **TextCNN**,  \n",
    "- **biGRU**,  \n",
    "- **FastText-style**,  \n",
    "а также модификация последней с использованием предобученных эмбеддингов **GloVe**.\n",
    "\n",
    "Размер обучающего набора составил 50 000 строк, со сбалансированным таргетом. В качестве метрик использовались accuracy и F1 score.\n",
    "\n",
    "#### Результаты после 20 эпох обучения\n",
    "\n",
    "| Модель                         | Accuracy | F1 Score |\n",
    "|-------------------------------|----------|----------|\n",
    "| TextCNN                       | 0.5403   | 0.5533   |\n",
    "| BiGRU                         | 0.5286   | 0.5365   |\n",
    "| FastText-style                | 0.5347   | 0.5484   |\n",
    "| FastText-style + GloVe        | 0.5335   | 0.5331   |\n",
    "\n",
    "\n",
    "#### Анализ полученных результатов\n",
    "\n",
    "Полученные значения метрик (accuracy около 53–54%, F1 score - до 55%) можно считать ожидаемыми для задачи высокой сложности. Финансовые тексты, как правило, содержат много шума, неоднозначностей, специфической терминологии и не всегда напрямую связаны с движением цены. Кроме того, даже для человека предсказание реакции рынка на новости зачастую затруднительно.\n",
    "\n",
    "**Цель данного этапа достигнута**:  \n",
    "- Продемонстрирована возможность применения моделей глубинного обучения для анализа финансовых текстов.  \n",
    "- Выстроен процесс предобработки текстовых данных.  \n",
    "- Подтверждено, что даже простые архитектуры глубинного обучения более чувствительны к структуре и смыслу текстов, чем классические ML-модели, что находит отражение в улучшенных значениях метрик качества.\n",
    "\n",
    "#### Возможности для дальнейших улучшений\n",
    "\n",
    "При наличии вычислительных и временных ресурсов возможны следующие направления развития:  \n",
    "- Предобучение языковых моделей (например, BERT, FinBERT, RoBERTa) на собственных финансовых данных.  \n",
    "- Введение дополнительных рыночных признаков (волатильность, объёмы торгов, технические индикаторы).  \n",
    "- Применение мультимодальных моделей, объединяющих текстовые данные, графики и временные ряды.\n",
    "\n",
    "Методы глубинного обучения показали большую эффективность в задаче предсказания направления изменения цены акций на основе финансовых новостей. Полученные результаты свидетельствуют о  наличии потенциала дальнейшего улучшения качества классификации при применении более сложных архитектур и расширении объёма обучающих данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
