{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425ebb89-7618-4eaa-b990-e779b6f29270",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Этап 1** (проводился в Colab, здесь только код): суммаризация текстов новостей. Самая времяемкая часть работы, суммаризация для 10 000 новостей заняла 4 часа в Colab с использованием GPU, по итогам сформирован отдельный датасет, включающий исходные данные + колонку с суммаризацией, для дальнейших экспериментов. Для суммаризации использовалась модель-transformer BART из библиотеки Hugging Face:\n",
    "https://huggingface.co/facebook/bart-large-cnn\n",
    "Итог: файл sc454k_10k_summary_rs42.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ddf32-6bed-4bfc-8fc9-d75510a7712d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 1: Summarization (completed) -> sc454k_10k_summary_rs42.parquet\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "\n",
    "def summarize_text(text):\n",
    "    if len(text) > 1024:\n",
    "        text = text[:1024]\n",
    "    return summarizer(text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "tqdm.pandas(desc=\"Processing articles\")\n",
    "\n",
    "df['summary'] = df['Article'].progress_apply(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfa872b-784f-4898-b13a-3129ca6a2b91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Device Name: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f5b4fc-8bba-4769-b7bb-1831f729626b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 31 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Symbol                 10000 non-null  object \n",
      " 1   Security               10000 non-null  object \n",
      " 2   Sector                 9889 non-null   object \n",
      " 3   Industry               9889 non-null   object \n",
      " 4   URL                    10000 non-null  object \n",
      " 5   Date                   10000 non-null  object \n",
      " 6   RelatedStocksList      7302 non-null   object \n",
      " 7   Article                10000 non-null  object \n",
      " 8   Title                  9987 non-null   object \n",
      " 9   articleType            10000 non-null  object \n",
      " 10  Publication            9994 non-null   object \n",
      " 11  Author                 7003 non-null   object \n",
      " 12  weighted_avg_-96_hrs   10000 non-null  float64\n",
      " 13  weighted_avg_-48_hrs   10000 non-null  float64\n",
      " 14  weighted_avg_-24_hrs   10000 non-null  float64\n",
      " 15  weighted_avg_0_hrs     10000 non-null  float64\n",
      " 16  weighted_avg_0.25_hrs  10000 non-null  float64\n",
      " 17  weighted_avg_0.50_hrs  10000 non-null  float64\n",
      " 18  weighted_avg_1_hrs     10000 non-null  float64\n",
      " 19  weighted_avg_2_hrs     10000 non-null  float64\n",
      " 20  weighted_avg_4_hrs     10000 non-null  float64\n",
      " 21  weighted_avg_6_hrs     10000 non-null  float64\n",
      " 22  weighted_avg_8_hrs     10000 non-null  float64\n",
      " 23  weighted_avg_12_hrs    10000 non-null  float64\n",
      " 24  weighted_avg_24_hrs    10000 non-null  float64\n",
      " 25  weighted_avg_48_hrs    10000 non-null  float64\n",
      " 26  weighted_avg_72_hrs    10000 non-null  float64\n",
      " 27  weighted_avg_96_hrs    10000 non-null  float64\n",
      " 28  weighted_avg_360_hrs   10000 non-null  float64\n",
      " 29  weighted_avg_720_hrs   10000 non-null  float64\n",
      " 30  summary                10000 non-null  object \n",
      "dtypes: float64(18), object(13)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>RelatedStocksList</th>\n",
       "      <th>Article</th>\n",
       "      <th>Title</th>\n",
       "      <th>articleType</th>\n",
       "      <th>...</th>\n",
       "      <th>weighted_avg_6_hrs</th>\n",
       "      <th>weighted_avg_8_hrs</th>\n",
       "      <th>weighted_avg_12_hrs</th>\n",
       "      <th>weighted_avg_24_hrs</th>\n",
       "      <th>weighted_avg_48_hrs</th>\n",
       "      <th>weighted_avg_72_hrs</th>\n",
       "      <th>weighted_avg_96_hrs</th>\n",
       "      <th>weighted_avg_360_hrs</th>\n",
       "      <th>weighted_avg_720_hrs</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PZZA</td>\n",
       "      <td>Papa John's International, Inc.</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>https://www.nasdaq.com/articles/papa-johns-sha...</td>\n",
       "      <td>Jun 18, 2019 07:39 PM ET</td>\n",
       "      <td>Markets</td>\n",
       "      <td>A **Papa John’s** (NASDAQ:) partnership with S...</td>\n",
       "      <td>Papa John’s Shaq Deal: 7 Things About Shaquill...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>49.4716</td>\n",
       "      <td>48.9466</td>\n",
       "      <td>49.1189</td>\n",
       "      <td>49.0284</td>\n",
       "      <td>49.0755</td>\n",
       "      <td>46.8525</td>\n",
       "      <td>44.1457</td>\n",
       "      <td>44.7833</td>\n",
       "      <td>45.1615</td>\n",
       "      <td>PZZA stock is down 2.4% Tuesday. NBA legend Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOVE</td>\n",
       "      <td>The Lovesac Company</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Other Specialty Stores</td>\n",
       "      <td>https://www.nasdaq.com/articles/monday-10-17-i...</td>\n",
       "      <td>Oct 17, 2022 02:45 PM ET</td>\n",
       "      <td>FRD|Markets</td>\n",
       "      <td>Bargain hunters are wise to pay careful attent...</td>\n",
       "      <td>Monday 10/17 Insider Buying Report: FRD, LOVE</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0903</td>\n",
       "      <td>21.8352</td>\n",
       "      <td>22.5121</td>\n",
       "      <td>22.8110</td>\n",
       "      <td>22.1818</td>\n",
       "      <td>21.3805</td>\n",
       "      <td>21.1099</td>\n",
       "      <td>24.7275</td>\n",
       "      <td>26.6617</td>\n",
       "      <td>Bargain hunters are wise to pay careful attent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RCKT</td>\n",
       "      <td>Rocket Pharmaceuticals, Inc.</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology: Pharmaceutical Preparations</td>\n",
       "      <td>https://www.nasdaq.com/press-release/rocket-ph...</td>\n",
       "      <td>Feb 09, 2023 04:07 PM ET</td>\n",
       "      <td>None</td>\n",
       "      <td>CRANBURY, N.J.--(BUSINESS WIRE)--\\n [Rocket Ph...</td>\n",
       "      <td>Rocket Pharmaceuticals to Present at the SVB S...</td>\n",
       "      <td>Press Release</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0081</td>\n",
       "      <td>20.2062</td>\n",
       "      <td>19.7100</td>\n",
       "      <td>19.6557</td>\n",
       "      <td>20.0553</td>\n",
       "      <td>19.8299</td>\n",
       "      <td>19.8000</td>\n",
       "      <td>18.8052</td>\n",
       "      <td>14.8639</td>\n",
       "      <td>Rocket Pharmaceuticals, Inc. is a leading late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMBL</td>\n",
       "      <td>Bumble Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Software: Programming Data Processing</td>\n",
       "      <td>https://www.nasdaq.com/articles/commit-to-purc...</td>\n",
       "      <td>Jun 27, 2023 01:35 PM ET</td>\n",
       "      <td>Markets</td>\n",
       "      <td>Investors eyeing a purchase of Bumble Inc (Sym...</td>\n",
       "      <td>Commit To Purchase Bumble At $10, Earn 12.5% U...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>16.8340</td>\n",
       "      <td>16.4434</td>\n",
       "      <td>16.8182</td>\n",
       "      <td>16.7866</td>\n",
       "      <td>16.7633</td>\n",
       "      <td>16.8915</td>\n",
       "      <td>16.8013</td>\n",
       "      <td>18.8289</td>\n",
       "      <td>18.7471</td>\n",
       "      <td>Selling a put does not give an investor access...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSIQ</td>\n",
       "      <td>Canadian Solar Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>https://www.nasdaq.com/articles/jinkosolar-jks...</td>\n",
       "      <td>Jan 15, 2023 02:19 AM ET</td>\n",
       "      <td>Stocks|SOL|JKS|SEDG</td>\n",
       "      <td>**JinkoSolar Holding Co., Ltd**. [JKS](https:/...</td>\n",
       "      <td>JinkoSolar (JKS) Introduces Upgraded Tiger Neo...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>42.6765</td>\n",
       "      <td>42.6765</td>\n",
       "      <td>42.4230</td>\n",
       "      <td>42.4230</td>\n",
       "      <td>42.4461</td>\n",
       "      <td>41.6878</td>\n",
       "      <td>40.2684</td>\n",
       "      <td>38.7182</td>\n",
       "      <td>40.5397</td>\n",
       "      <td>JinkoSolar Holding Co., Ltd. (JKS) recently un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                         Security                  Sector  \\\n",
       "0   PZZA  Papa John's International, Inc.  Consumer Discretionary   \n",
       "1   LOVE              The Lovesac Company  Consumer Discretionary   \n",
       "2   RCKT     Rocket Pharmaceuticals, Inc.             Health Care   \n",
       "3   BMBL                      Bumble Inc.              Technology   \n",
       "4   CSIQ              Canadian Solar Inc.              Technology   \n",
       "\n",
       "                                         Industry  \\\n",
       "0                                     Restaurants   \n",
       "1                          Other Specialty Stores   \n",
       "2      Biotechnology: Pharmaceutical Preparations   \n",
       "3  Computer Software: Programming Data Processing   \n",
       "4                                  Semiconductors   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.nasdaq.com/articles/papa-johns-sha...   \n",
       "1  https://www.nasdaq.com/articles/monday-10-17-i...   \n",
       "2  https://www.nasdaq.com/press-release/rocket-ph...   \n",
       "3  https://www.nasdaq.com/articles/commit-to-purc...   \n",
       "4  https://www.nasdaq.com/articles/jinkosolar-jks...   \n",
       "\n",
       "                       Date    RelatedStocksList  \\\n",
       "0  Jun 18, 2019 07:39 PM ET              Markets   \n",
       "1  Oct 17, 2022 02:45 PM ET          FRD|Markets   \n",
       "2  Feb 09, 2023 04:07 PM ET                 None   \n",
       "3  Jun 27, 2023 01:35 PM ET              Markets   \n",
       "4  Jan 15, 2023 02:19 AM ET  Stocks|SOL|JKS|SEDG   \n",
       "\n",
       "                                             Article  \\\n",
       "0  A **Papa John’s** (NASDAQ:) partnership with S...   \n",
       "1  Bargain hunters are wise to pay careful attent...   \n",
       "2  CRANBURY, N.J.--(BUSINESS WIRE)--\\n [Rocket Ph...   \n",
       "3  Investors eyeing a purchase of Bumble Inc (Sym...   \n",
       "4  **JinkoSolar Holding Co., Ltd**. [JKS](https:/...   \n",
       "\n",
       "                                               Title    articleType  ...  \\\n",
       "0  Papa John’s Shaq Deal: 7 Things About Shaquill...           News  ...   \n",
       "1      Monday 10/17 Insider Buying Report: FRD, LOVE           News  ...   \n",
       "2  Rocket Pharmaceuticals to Present at the SVB S...  Press Release  ...   \n",
       "3  Commit To Purchase Bumble At $10, Earn 12.5% U...           News  ...   \n",
       "4  JinkoSolar (JKS) Introduces Upgraded Tiger Neo...           News  ...   \n",
       "\n",
       "  weighted_avg_6_hrs weighted_avg_8_hrs  weighted_avg_12_hrs  \\\n",
       "0            49.4716            48.9466              49.1189   \n",
       "1            21.0903            21.8352              22.5121   \n",
       "2            20.0081            20.2062              19.7100   \n",
       "3            16.8340            16.4434              16.8182   \n",
       "4            42.6765            42.6765              42.4230   \n",
       "\n",
       "   weighted_avg_24_hrs  weighted_avg_48_hrs  weighted_avg_72_hrs  \\\n",
       "0              49.0284              49.0755              46.8525   \n",
       "1              22.8110              22.1818              21.3805   \n",
       "2              19.6557              20.0553              19.8299   \n",
       "3              16.7866              16.7633              16.8915   \n",
       "4              42.4230              42.4461              41.6878   \n",
       "\n",
       "   weighted_avg_96_hrs  weighted_avg_360_hrs  weighted_avg_720_hrs  \\\n",
       "0              44.1457               44.7833               45.1615   \n",
       "1              21.1099               24.7275               26.6617   \n",
       "2              19.8000               18.8052               14.8639   \n",
       "3              16.8013               18.8289               18.7471   \n",
       "4              40.2684               38.7182               40.5397   \n",
       "\n",
       "                                             summary  \n",
       "0  PZZA stock is down 2.4% Tuesday. NBA legend Sh...  \n",
       "1  Bargain hunters are wise to pay careful attent...  \n",
       "2  Rocket Pharmaceuticals, Inc. is a leading late...  \n",
       "3  Selling a put does not give an investor access...  \n",
       "4  JinkoSolar Holding Co., Ltd. (JKS) recently un...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверим результаты суммаризации\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"sc454k_10k_summary_rs42.parquet\"\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "display(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12aaa1-7142-492a-8070-968b56ca34aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Этап 2** Sentiment analysis для текстов summary. Итог: файлsc454k_10k_summary_sentiment_rs42.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adaf45ce-302b-4682-b609-8171b5854db4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f334d45b507945fd9de005d84109de01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing sentiment:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed. Results saved to sc454k_10k_summary_sentiment_rs42.parquet\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Sentiment analysis for summary (completed) -> sc454k_10k_summary_sentiment_rs42.parquet\n",
    "\n",
    "output_file = \"sc454k_10k_summary_sentiment_rs42.parquet\"\n",
    "\n",
    "if 'summary' not in df.columns:\n",
    "    raise ValueError(\"The 'summary' column is missing in the input file. Ensure Step 1 is completed properly.\")\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    results = sentiment_analyzer(text)\n",
    "    result = results[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "\n",
    "    if label == \"positive\":\n",
    "        positive_prob = score\n",
    "        negative_prob = 1 - score\n",
    "    elif label == \"negative\":\n",
    "        negative_prob = score\n",
    "        positive_prob = 1 - score\n",
    "    else:  # Neutral case\n",
    "        positive_prob = 0.5\n",
    "        negative_prob = 0.5\n",
    "\n",
    "    return positive_prob, negative_prob\n",
    "\n",
    "tqdm.pandas(desc=\"Analyzing sentiment\")\n",
    "\n",
    "df[['positive_prob', 'negative_prob']] = df['summary'].progress_apply(\n",
    "    lambda text: pd.Series(analyze_sentiment(text))\n",
    ")\n",
    "\n",
    "df.to_parquet(output_file, index=False)\n",
    "print(f\"Sentiment analysis completed. Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac9e16-d23a-4643-b02e-06b9171c6ce5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Этап 3**: Получение эмбеддингов для summary. Итог: файл sc454k_10k_summary_embeddings_sentiment_rs42.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b3ab0106-d50c-4f12-b4b8-824624f0968c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT model loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5e473f5560449b9be035e2244fec47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating FinBERT embeddings:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with embeddings saved to sc454k_10k_summary_embeddings_sentiment_rs42.parquet\n"
     ]
    }
   ],
   "source": [
    "# Stage 3: Embeddings for summary (completed) -> sc454k_10k_summary_embeddings_sentiment_rs42.parquet\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "output_file = \"sc454k_10k_summary_embeddings_sentiment_rs42.parquet\"\n",
    "\n",
    "if 'summary' not in df.columns:\n",
    "    raise ValueError(\"The 'summary' column is missing in the input file. Ensure Step 1 is completed properly.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "print(\"FinBERT model loaded.\")\n",
    "\n",
    "\n",
    "def generate_embeddings(texts, tokenizer, model, output_dim=64):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Generating FinBERT embeddings\"):\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(cls_embedding.flatten())\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # Apply PCA to reduce to output_dim\n",
    "    pca = PCA(n_components=output_dim, random_state=42)\n",
    "    reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    return reduced_embeddings\n",
    "\n",
    "# Apply the embedding function to the 'summary' column\n",
    "embeddings_num = 16 # 64\n",
    "summaries = df['summary'].tolist()\n",
    "reduced_embeddings = generate_embeddings(summaries, tokenizer, model, output_dim=embeddings_num)\n",
    "\n",
    "# Add embeddings as new columns in the existing DataFrame\n",
    "for i in range(embeddings_num):\n",
    "    df[f'embedding_{i}'] = reduced_embeddings[:, i]\n",
    "\n",
    "df.to_parquet(output_file, index=False)\n",
    "print(f\"Updated DataFrame with embeddings saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4800e1a7-5772-4557-8461-a9d3448f7c6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 97 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Symbol                 10000 non-null  object \n",
      " 1   Security               10000 non-null  object \n",
      " 2   Sector                 9889 non-null   object \n",
      " 3   Industry               9889 non-null   object \n",
      " 4   URL                    10000 non-null  object \n",
      " 5   Date                   10000 non-null  object \n",
      " 6   RelatedStocksList      7302 non-null   object \n",
      " 7   Article                10000 non-null  object \n",
      " 8   Title                  9987 non-null   object \n",
      " 9   articleType            10000 non-null  object \n",
      " 10  Publication            9994 non-null   object \n",
      " 11  Author                 7003 non-null   object \n",
      " 12  weighted_avg_-96_hrs   10000 non-null  float64\n",
      " 13  weighted_avg_-48_hrs   10000 non-null  float64\n",
      " 14  weighted_avg_-24_hrs   10000 non-null  float64\n",
      " 15  weighted_avg_0_hrs     10000 non-null  float64\n",
      " 16  weighted_avg_0.25_hrs  10000 non-null  float64\n",
      " 17  weighted_avg_0.50_hrs  10000 non-null  float64\n",
      " 18  weighted_avg_1_hrs     10000 non-null  float64\n",
      " 19  weighted_avg_2_hrs     10000 non-null  float64\n",
      " 20  weighted_avg_4_hrs     10000 non-null  float64\n",
      " 21  weighted_avg_6_hrs     10000 non-null  float64\n",
      " 22  weighted_avg_8_hrs     10000 non-null  float64\n",
      " 23  weighted_avg_12_hrs    10000 non-null  float64\n",
      " 24  weighted_avg_24_hrs    10000 non-null  float64\n",
      " 25  weighted_avg_48_hrs    10000 non-null  float64\n",
      " 26  weighted_avg_72_hrs    10000 non-null  float64\n",
      " 27  weighted_avg_96_hrs    10000 non-null  float64\n",
      " 28  weighted_avg_360_hrs   10000 non-null  float64\n",
      " 29  weighted_avg_720_hrs   10000 non-null  float64\n",
      " 30  summary                10000 non-null  object \n",
      " 31  positive_prob          10000 non-null  float64\n",
      " 32  negative_prob          10000 non-null  float64\n",
      " 33  embedding_0            10000 non-null  float32\n",
      " 34  embedding_1            10000 non-null  float32\n",
      " 35  embedding_2            10000 non-null  float32\n",
      " 36  embedding_3            10000 non-null  float32\n",
      " 37  embedding_4            10000 non-null  float32\n",
      " 38  embedding_5            10000 non-null  float32\n",
      " 39  embedding_6            10000 non-null  float32\n",
      " 40  embedding_7            10000 non-null  float32\n",
      " 41  embedding_8            10000 non-null  float32\n",
      " 42  embedding_9            10000 non-null  float32\n",
      " 43  embedding_10           10000 non-null  float32\n",
      " 44  embedding_11           10000 non-null  float32\n",
      " 45  embedding_12           10000 non-null  float32\n",
      " 46  embedding_13           10000 non-null  float32\n",
      " 47  embedding_14           10000 non-null  float32\n",
      " 48  embedding_15           10000 non-null  float32\n",
      " 49  embedding_16           10000 non-null  float32\n",
      " 50  embedding_17           10000 non-null  float32\n",
      " 51  embedding_18           10000 non-null  float32\n",
      " 52  embedding_19           10000 non-null  float32\n",
      " 53  embedding_20           10000 non-null  float32\n",
      " 54  embedding_21           10000 non-null  float32\n",
      " 55  embedding_22           10000 non-null  float32\n",
      " 56  embedding_23           10000 non-null  float32\n",
      " 57  embedding_24           10000 non-null  float32\n",
      " 58  embedding_25           10000 non-null  float32\n",
      " 59  embedding_26           10000 non-null  float32\n",
      " 60  embedding_27           10000 non-null  float32\n",
      " 61  embedding_28           10000 non-null  float32\n",
      " 62  embedding_29           10000 non-null  float32\n",
      " 63  embedding_30           10000 non-null  float32\n",
      " 64  embedding_31           10000 non-null  float32\n",
      " 65  embedding_32           10000 non-null  float32\n",
      " 66  embedding_33           10000 non-null  float32\n",
      " 67  embedding_34           10000 non-null  float32\n",
      " 68  embedding_35           10000 non-null  float32\n",
      " 69  embedding_36           10000 non-null  float32\n",
      " 70  embedding_37           10000 non-null  float32\n",
      " 71  embedding_38           10000 non-null  float32\n",
      " 72  embedding_39           10000 non-null  float32\n",
      " 73  embedding_40           10000 non-null  float32\n",
      " 74  embedding_41           10000 non-null  float32\n",
      " 75  embedding_42           10000 non-null  float32\n",
      " 76  embedding_43           10000 non-null  float32\n",
      " 77  embedding_44           10000 non-null  float32\n",
      " 78  embedding_45           10000 non-null  float32\n",
      " 79  embedding_46           10000 non-null  float32\n",
      " 80  embedding_47           10000 non-null  float32\n",
      " 81  embedding_48           10000 non-null  float32\n",
      " 82  embedding_49           10000 non-null  float32\n",
      " 83  embedding_50           10000 non-null  float32\n",
      " 84  embedding_51           10000 non-null  float32\n",
      " 85  embedding_52           10000 non-null  float32\n",
      " 86  embedding_53           10000 non-null  float32\n",
      " 87  embedding_54           10000 non-null  float32\n",
      " 88  embedding_55           10000 non-null  float32\n",
      " 89  embedding_56           10000 non-null  float32\n",
      " 90  embedding_57           10000 non-null  float32\n",
      " 91  embedding_58           10000 non-null  float32\n",
      " 92  embedding_59           10000 non-null  float32\n",
      " 93  embedding_60           10000 non-null  float32\n",
      " 94  embedding_61           10000 non-null  float32\n",
      " 95  embedding_62           10000 non-null  float32\n",
      " 96  embedding_63           10000 non-null  float32\n",
      "dtypes: float32(64), float64(20), object(13)\n",
      "memory usage: 5.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>RelatedStocksList</th>\n",
       "      <th>Article</th>\n",
       "      <th>Title</th>\n",
       "      <th>articleType</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_54</th>\n",
       "      <th>embedding_55</th>\n",
       "      <th>embedding_56</th>\n",
       "      <th>embedding_57</th>\n",
       "      <th>embedding_58</th>\n",
       "      <th>embedding_59</th>\n",
       "      <th>embedding_60</th>\n",
       "      <th>embedding_61</th>\n",
       "      <th>embedding_62</th>\n",
       "      <th>embedding_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PZZA</td>\n",
       "      <td>Papa John's International, Inc.</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>https://www.nasdaq.com/articles/papa-johns-sha...</td>\n",
       "      <td>Jun 18, 2019 07:39 PM ET</td>\n",
       "      <td>Markets</td>\n",
       "      <td>A **Papa John’s** (NASDAQ:) partnership with S...</td>\n",
       "      <td>Papa John’s Shaq Deal: 7 Things About Shaquill...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428295</td>\n",
       "      <td>2.006473</td>\n",
       "      <td>0.394459</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>-0.588683</td>\n",
       "      <td>0.261005</td>\n",
       "      <td>-0.277107</td>\n",
       "      <td>-0.401201</td>\n",
       "      <td>0.993497</td>\n",
       "      <td>-1.298946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOVE</td>\n",
       "      <td>The Lovesac Company</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Other Specialty Stores</td>\n",
       "      <td>https://www.nasdaq.com/articles/monday-10-17-i...</td>\n",
       "      <td>Oct 17, 2022 02:45 PM ET</td>\n",
       "      <td>FRD|Markets</td>\n",
       "      <td>Bargain hunters are wise to pay careful attent...</td>\n",
       "      <td>Monday 10/17 Insider Buying Report: FRD, LOVE</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373199</td>\n",
       "      <td>0.129881</td>\n",
       "      <td>-0.472512</td>\n",
       "      <td>-0.091278</td>\n",
       "      <td>-0.054997</td>\n",
       "      <td>0.237271</td>\n",
       "      <td>1.030590</td>\n",
       "      <td>0.954953</td>\n",
       "      <td>-0.402630</td>\n",
       "      <td>0.109326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RCKT</td>\n",
       "      <td>Rocket Pharmaceuticals, Inc.</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology: Pharmaceutical Preparations</td>\n",
       "      <td>https://www.nasdaq.com/press-release/rocket-ph...</td>\n",
       "      <td>Feb 09, 2023 04:07 PM ET</td>\n",
       "      <td>None</td>\n",
       "      <td>CRANBURY, N.J.--(BUSINESS WIRE)--\\n [Rocket Ph...</td>\n",
       "      <td>Rocket Pharmaceuticals to Present at the SVB S...</td>\n",
       "      <td>Press Release</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828223</td>\n",
       "      <td>0.039730</td>\n",
       "      <td>-0.057006</td>\n",
       "      <td>-0.236145</td>\n",
       "      <td>0.212412</td>\n",
       "      <td>-0.291634</td>\n",
       "      <td>-0.168695</td>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.324847</td>\n",
       "      <td>0.354604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BMBL</td>\n",
       "      <td>Bumble Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Software: Programming Data Processing</td>\n",
       "      <td>https://www.nasdaq.com/articles/commit-to-purc...</td>\n",
       "      <td>Jun 27, 2023 01:35 PM ET</td>\n",
       "      <td>Markets</td>\n",
       "      <td>Investors eyeing a purchase of Bumble Inc (Sym...</td>\n",
       "      <td>Commit To Purchase Bumble At $10, Earn 12.5% U...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093483</td>\n",
       "      <td>-0.643263</td>\n",
       "      <td>-0.205195</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>-0.174820</td>\n",
       "      <td>0.320681</td>\n",
       "      <td>0.322185</td>\n",
       "      <td>-0.346533</td>\n",
       "      <td>-0.240105</td>\n",
       "      <td>-0.264372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSIQ</td>\n",
       "      <td>Canadian Solar Inc.</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>https://www.nasdaq.com/articles/jinkosolar-jks...</td>\n",
       "      <td>Jan 15, 2023 02:19 AM ET</td>\n",
       "      <td>Stocks|SOL|JKS|SEDG</td>\n",
       "      <td>**JinkoSolar Holding Co., Ltd**. [JKS](https:/...</td>\n",
       "      <td>JinkoSolar (JKS) Introduces Upgraded Tiger Neo...</td>\n",
       "      <td>News</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045486</td>\n",
       "      <td>-0.707836</td>\n",
       "      <td>-0.528663</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.113261</td>\n",
       "      <td>0.347916</td>\n",
       "      <td>0.209496</td>\n",
       "      <td>0.633640</td>\n",
       "      <td>-0.182710</td>\n",
       "      <td>-0.004339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                         Security                  Sector  \\\n",
       "0   PZZA  Papa John's International, Inc.  Consumer Discretionary   \n",
       "1   LOVE              The Lovesac Company  Consumer Discretionary   \n",
       "2   RCKT     Rocket Pharmaceuticals, Inc.             Health Care   \n",
       "3   BMBL                      Bumble Inc.              Technology   \n",
       "4   CSIQ              Canadian Solar Inc.              Technology   \n",
       "\n",
       "                                         Industry  \\\n",
       "0                                     Restaurants   \n",
       "1                          Other Specialty Stores   \n",
       "2      Biotechnology: Pharmaceutical Preparations   \n",
       "3  Computer Software: Programming Data Processing   \n",
       "4                                  Semiconductors   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.nasdaq.com/articles/papa-johns-sha...   \n",
       "1  https://www.nasdaq.com/articles/monday-10-17-i...   \n",
       "2  https://www.nasdaq.com/press-release/rocket-ph...   \n",
       "3  https://www.nasdaq.com/articles/commit-to-purc...   \n",
       "4  https://www.nasdaq.com/articles/jinkosolar-jks...   \n",
       "\n",
       "                       Date    RelatedStocksList  \\\n",
       "0  Jun 18, 2019 07:39 PM ET              Markets   \n",
       "1  Oct 17, 2022 02:45 PM ET          FRD|Markets   \n",
       "2  Feb 09, 2023 04:07 PM ET                 None   \n",
       "3  Jun 27, 2023 01:35 PM ET              Markets   \n",
       "4  Jan 15, 2023 02:19 AM ET  Stocks|SOL|JKS|SEDG   \n",
       "\n",
       "                                             Article  \\\n",
       "0  A **Papa John’s** (NASDAQ:) partnership with S...   \n",
       "1  Bargain hunters are wise to pay careful attent...   \n",
       "2  CRANBURY, N.J.--(BUSINESS WIRE)--\\n [Rocket Ph...   \n",
       "3  Investors eyeing a purchase of Bumble Inc (Sym...   \n",
       "4  **JinkoSolar Holding Co., Ltd**. [JKS](https:/...   \n",
       "\n",
       "                                               Title    articleType  ...  \\\n",
       "0  Papa John’s Shaq Deal: 7 Things About Shaquill...           News  ...   \n",
       "1      Monday 10/17 Insider Buying Report: FRD, LOVE           News  ...   \n",
       "2  Rocket Pharmaceuticals to Present at the SVB S...  Press Release  ...   \n",
       "3  Commit To Purchase Bumble At $10, Earn 12.5% U...           News  ...   \n",
       "4  JinkoSolar (JKS) Introduces Upgraded Tiger Neo...           News  ...   \n",
       "\n",
       "  embedding_54 embedding_55  embedding_56  embedding_57  embedding_58  \\\n",
       "0    -0.428295     2.006473      0.394459     -1.294207     -0.588683   \n",
       "1     0.373199     0.129881     -0.472512     -0.091278     -0.054997   \n",
       "2     0.828223     0.039730     -0.057006     -0.236145      0.212412   \n",
       "3    -0.093483    -0.643263     -0.205195      0.656000     -0.174820   \n",
       "4     0.045486    -0.707836     -0.528663      0.043703      0.113261   \n",
       "\n",
       "   embedding_59  embedding_60  embedding_61  embedding_62  embedding_63  \n",
       "0      0.261005     -0.277107     -0.401201      0.993497     -1.298946  \n",
       "1      0.237271      1.030590      0.954953     -0.402630      0.109326  \n",
       "2     -0.291634     -0.168695      0.052711      0.324847      0.354604  \n",
       "3      0.320681      0.322185     -0.346533     -0.240105     -0.264372  \n",
       "4      0.347916      0.209496      0.633640     -0.182710     -0.004339  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919de35f-a216-4d9c-8260-6b9d6b84ddbd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Этап 4**: Эксперименты с линейной регрессией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df4ed064-8b63-4bb1-b0f0-6e3ad5254117",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_56</th>\n",
       "      <th>embedding_57</th>\n",
       "      <th>embedding_58</th>\n",
       "      <th>embedding_59</th>\n",
       "      <th>embedding_60</th>\n",
       "      <th>embedding_61</th>\n",
       "      <th>embedding_62</th>\n",
       "      <th>embedding_63</th>\n",
       "      <th>positive_prob</th>\n",
       "      <th>negative_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.851347</td>\n",
       "      <td>3.120531</td>\n",
       "      <td>-1.313104</td>\n",
       "      <td>1.097990</td>\n",
       "      <td>0.165416</td>\n",
       "      <td>-3.166709</td>\n",
       "      <td>-1.919456</td>\n",
       "      <td>-3.700295</td>\n",
       "      <td>1.397489</td>\n",
       "      <td>2.963977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394459</td>\n",
       "      <td>-1.294207</td>\n",
       "      <td>-0.588683</td>\n",
       "      <td>0.261005</td>\n",
       "      <td>-0.277107</td>\n",
       "      <td>-0.401201</td>\n",
       "      <td>0.993497</td>\n",
       "      <td>-1.298946</td>\n",
       "      <td>0.434472</td>\n",
       "      <td>0.565528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.717551</td>\n",
       "      <td>5.867550</td>\n",
       "      <td>-1.688272</td>\n",
       "      <td>0.512769</td>\n",
       "      <td>1.687632</td>\n",
       "      <td>-2.153067</td>\n",
       "      <td>-0.399103</td>\n",
       "      <td>-0.039187</td>\n",
       "      <td>-1.714151</td>\n",
       "      <td>-0.247120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472512</td>\n",
       "      <td>-0.091278</td>\n",
       "      <td>-0.054997</td>\n",
       "      <td>0.237271</td>\n",
       "      <td>1.030590</td>\n",
       "      <td>0.954953</td>\n",
       "      <td>-0.402630</td>\n",
       "      <td>0.109326</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.450830</td>\n",
       "      <td>4.293439</td>\n",
       "      <td>2.219385</td>\n",
       "      <td>-1.031255</td>\n",
       "      <td>0.517836</td>\n",
       "      <td>1.113833</td>\n",
       "      <td>-1.125759</td>\n",
       "      <td>-1.134538</td>\n",
       "      <td>-0.453618</td>\n",
       "      <td>-0.098010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057006</td>\n",
       "      <td>-0.236145</td>\n",
       "      <td>0.212412</td>\n",
       "      <td>-0.291634</td>\n",
       "      <td>-0.168695</td>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.324847</td>\n",
       "      <td>0.354604</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.271941</td>\n",
       "      <td>3.637712</td>\n",
       "      <td>-5.173738</td>\n",
       "      <td>0.358423</td>\n",
       "      <td>-1.138867</td>\n",
       "      <td>-3.150231</td>\n",
       "      <td>1.813922</td>\n",
       "      <td>2.004270</td>\n",
       "      <td>-2.041636</td>\n",
       "      <td>-0.370303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205195</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>-0.174820</td>\n",
       "      <td>0.320681</td>\n",
       "      <td>0.322185</td>\n",
       "      <td>-0.346533</td>\n",
       "      <td>-0.240105</td>\n",
       "      <td>-0.264372</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.795382</td>\n",
       "      <td>-3.835295</td>\n",
       "      <td>3.250404</td>\n",
       "      <td>-3.269783</td>\n",
       "      <td>-1.900187</td>\n",
       "      <td>-1.254677</td>\n",
       "      <td>-2.295327</td>\n",
       "      <td>0.176403</td>\n",
       "      <td>-1.355946</td>\n",
       "      <td>-0.118056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528663</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.113261</td>\n",
       "      <td>0.347916</td>\n",
       "      <td>0.209496</td>\n",
       "      <td>0.633640</td>\n",
       "      <td>-0.182710</td>\n",
       "      <td>-0.004339</td>\n",
       "      <td>0.706716</td>\n",
       "      <td>0.293284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0    -4.851347     3.120531    -1.313104     1.097990     0.165416   \n",
       "1     7.717551     5.867550    -1.688272     0.512769     1.687632   \n",
       "2     9.450830     4.293439     2.219385    -1.031255     0.517836   \n",
       "3     5.271941     3.637712    -5.173738     0.358423    -1.138867   \n",
       "4     8.795382    -3.835295     3.250404    -3.269783    -1.900187   \n",
       "\n",
       "   embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0    -3.166709    -1.919456    -3.700295     1.397489     2.963977  ...   \n",
       "1    -2.153067    -0.399103    -0.039187    -1.714151    -0.247120  ...   \n",
       "2     1.113833    -1.125759    -1.134538    -0.453618    -0.098010  ...   \n",
       "3    -3.150231     1.813922     2.004270    -2.041636    -0.370303  ...   \n",
       "4    -1.254677    -2.295327     0.176403    -1.355946    -0.118056  ...   \n",
       "\n",
       "   embedding_56  embedding_57  embedding_58  embedding_59  embedding_60  \\\n",
       "0      0.394459     -1.294207     -0.588683      0.261005     -0.277107   \n",
       "1     -0.472512     -0.091278     -0.054997      0.237271      1.030590   \n",
       "2     -0.057006     -0.236145      0.212412     -0.291634     -0.168695   \n",
       "3     -0.205195      0.656000     -0.174820      0.320681      0.322185   \n",
       "4     -0.528663      0.043703      0.113261      0.347916      0.209496   \n",
       "\n",
       "   embedding_61  embedding_62  embedding_63  positive_prob  negative_prob  \n",
       "0     -0.401201      0.993497     -1.298946       0.434472       0.565528  \n",
       "1      0.954953     -0.402630      0.109326       0.500000       0.500000  \n",
       "2      0.052711      0.324847      0.354604       0.500000       0.500000  \n",
       "3     -0.346533     -0.240105     -0.264372       0.500000       0.500000  \n",
       "4      0.633640     -0.182710     -0.004339       0.706716       0.293284  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0   -0.72\n",
       "1    0.00\n",
       "2    0.07\n",
       "3   -0.02\n",
       "4    0.00\n",
       "Name: price_4h_change_percent, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9706 entries, 0 to 9999\n",
      "Data columns (total 66 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   embedding_0    9706 non-null   float32\n",
      " 1   embedding_1    9706 non-null   float32\n",
      " 2   embedding_2    9706 non-null   float32\n",
      " 3   embedding_3    9706 non-null   float32\n",
      " 4   embedding_4    9706 non-null   float32\n",
      " 5   embedding_5    9706 non-null   float32\n",
      " 6   embedding_6    9706 non-null   float32\n",
      " 7   embedding_7    9706 non-null   float32\n",
      " 8   embedding_8    9706 non-null   float32\n",
      " 9   embedding_9    9706 non-null   float32\n",
      " 10  embedding_10   9706 non-null   float32\n",
      " 11  embedding_11   9706 non-null   float32\n",
      " 12  embedding_12   9706 non-null   float32\n",
      " 13  embedding_13   9706 non-null   float32\n",
      " 14  embedding_14   9706 non-null   float32\n",
      " 15  embedding_15   9706 non-null   float32\n",
      " 16  embedding_16   9706 non-null   float32\n",
      " 17  embedding_17   9706 non-null   float32\n",
      " 18  embedding_18   9706 non-null   float32\n",
      " 19  embedding_19   9706 non-null   float32\n",
      " 20  embedding_20   9706 non-null   float32\n",
      " 21  embedding_21   9706 non-null   float32\n",
      " 22  embedding_22   9706 non-null   float32\n",
      " 23  embedding_23   9706 non-null   float32\n",
      " 24  embedding_24   9706 non-null   float32\n",
      " 25  embedding_25   9706 non-null   float32\n",
      " 26  embedding_26   9706 non-null   float32\n",
      " 27  embedding_27   9706 non-null   float32\n",
      " 28  embedding_28   9706 non-null   float32\n",
      " 29  embedding_29   9706 non-null   float32\n",
      " 30  embedding_30   9706 non-null   float32\n",
      " 31  embedding_31   9706 non-null   float32\n",
      " 32  embedding_32   9706 non-null   float32\n",
      " 33  embedding_33   9706 non-null   float32\n",
      " 34  embedding_34   9706 non-null   float32\n",
      " 35  embedding_35   9706 non-null   float32\n",
      " 36  embedding_36   9706 non-null   float32\n",
      " 37  embedding_37   9706 non-null   float32\n",
      " 38  embedding_38   9706 non-null   float32\n",
      " 39  embedding_39   9706 non-null   float32\n",
      " 40  embedding_40   9706 non-null   float32\n",
      " 41  embedding_41   9706 non-null   float32\n",
      " 42  embedding_42   9706 non-null   float32\n",
      " 43  embedding_43   9706 non-null   float32\n",
      " 44  embedding_44   9706 non-null   float32\n",
      " 45  embedding_45   9706 non-null   float32\n",
      " 46  embedding_46   9706 non-null   float32\n",
      " 47  embedding_47   9706 non-null   float32\n",
      " 48  embedding_48   9706 non-null   float32\n",
      " 49  embedding_49   9706 non-null   float32\n",
      " 50  embedding_50   9706 non-null   float32\n",
      " 51  embedding_51   9706 non-null   float32\n",
      " 52  embedding_52   9706 non-null   float32\n",
      " 53  embedding_53   9706 non-null   float32\n",
      " 54  embedding_54   9706 non-null   float32\n",
      " 55  embedding_55   9706 non-null   float32\n",
      " 56  embedding_56   9706 non-null   float32\n",
      " 57  embedding_57   9706 non-null   float32\n",
      " 58  embedding_58   9706 non-null   float32\n",
      " 59  embedding_59   9706 non-null   float32\n",
      " 60  embedding_60   9706 non-null   float32\n",
      " 61  embedding_61   9706 non-null   float32\n",
      " 62  embedding_62   9706 non-null   float32\n",
      " 63  embedding_63   9706 non-null   float32\n",
      " 64  positive_prob  9706 non-null   float64\n",
      " 65  negative_prob  9706 non-null   float64\n",
      "dtypes: float32(64), float64(2)\n",
      "memory usage: 2.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 9706 entries, 0 to 9999\n",
      "Series name: price_4h_change_percent\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "9706 non-null   float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 151.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 36.29354984352492\n",
      "R-squared (R2): -0.021691268963867527\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: Linreg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_parquet(\"sc454k_10k_summary_embeddings_sentiment_rs42.parquet\")\n",
    "\n",
    "df = df[df['weighted_avg_0_hrs'] > 0]\n",
    "\n",
    "# Recalculate price_24h_change_percent after filtering\n",
    "df['price_4h_change_percent'] = ((df['weighted_avg_4_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "# Drop rows with NaN values in the target or features\n",
    "df = df.dropna(subset=['price_4h_change_percent'])\n",
    "\n",
    "# Select features (embeddings + probabilities) and target\n",
    "feature_columns = [f\"embedding_{i}\" for i in range(64)] + [\"positive_prob\", \"negative_prob\"]\n",
    "X = df[feature_columns]\n",
    "y = df['price_4h_change_percent']\n",
    "\n",
    "display(X.head())\n",
    "display(y.head())\n",
    "display(X.info())\n",
    "display(y.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2c08c-63c1-4f59-8553-72093990dcc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы**: результаты неудовлетворительные, пробуем улучшить с помощью ансамблевого метода Gradient Boosting. \n",
    "Модель не линейная, применяется в качестве эксперимента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9f29244-570a-4b9a-8f97-806442bf548e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in i:\\soft\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in i:\\soft\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/124.9 MB 6.6 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 3.1/124.9 MB 9.7 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 5.5/124.9 MB 10.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 7.9/124.9 MB 10.8 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 10.5/124.9 MB 11.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 12.8/124.9 MB 11.2 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 15.5/124.9 MB 11.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 17.8/124.9 MB 11.4 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 20.2/124.9 MB 11.4 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 22.5/124.9 MB 11.4 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 24.9/124.9 MB 11.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 27.3/124.9 MB 11.5 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 29.9/124.9 MB 11.6 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 32.2/124.9 MB 11.6 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 34.6/124.9 MB 11.6 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 37.2/124.9 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 39.6/124.9 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 41.9/124.9 MB 11.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 44.3/124.9 MB 11.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 46.9/124.9 MB 11.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 49.3/124.9 MB 11.5 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 51.6/124.9 MB 11.6 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 54.0/124.9 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 56.6/124.9 MB 11.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 59.0/124.9 MB 11.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 61.3/124.9 MB 11.6 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 64.0/124.9 MB 11.6 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 66.3/124.9 MB 11.6 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 68.7/124.9 MB 11.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 71.3/124.9 MB 11.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 73.7/124.9 MB 11.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 76.0/124.9 MB 11.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 77.9/124.9 MB 11.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 80.5/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 83.1/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 85.5/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 87.8/124.9 MB 11.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 89.9/124.9 MB 11.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.5/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.9/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 97.3/124.9 MB 11.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.6/124.9 MB 11.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 102.0/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 104.6/124.9 MB 11.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 107.0/124.9 MB 11.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.3/124.9 MB 11.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 111.9/124.9 MB 11.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 114.0/124.9 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.7/124.9 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.0/124.9 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.4/124.9 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  123.7/124.9 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 11.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a38558e5-1285-440e-b9f1-0df05d6a26be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 696.075783830103\n",
      "R-squared (R2): -0.2786938547389144\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "df = pd.read_parquet('sc454k_10k_summary_embeddings_sentiment_rs42.parquet')\n",
    "\n",
    "# Add the target column\n",
    "df['price_24h_change_percent'] = ((df['weighted_avg_24_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "# Remove rows with infinite or NaN values in the target\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['price_24h_change_percent'])\n",
    "\n",
    "X = df[[col for col in df.columns if col.startswith('embedding_')] + ['positive_prob', 'negative_prob']]\n",
    "y = df['price_24h_change_percent']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Gradient Boosting model\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d576217-3c16-4e10-9f5f-49a75e3f5c08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы**: результаты по-прежнему неудовлетворительны.\n",
    "\n",
    "По итогам экспериментов с линейной регрессией:\n",
    "\n",
    "Задача регрессии для предсказания точных цен на основе финансовых новостей невыполнима из-за высокой сложности и многозначности финансовых рынков. Цены зависят не только от новостей, но и от множества других факторов, таких как макроэкономические данные, технические индикаторы, рыночные настроения и алгоритмическая торговля. Финансовые новости, как правило, дают только общий контекст, недостаточный для точного численного прогноза цены.\n",
    "\n",
    "**Этап 5** Переход к бинарной классификации \n",
    "\n",
    "Переход к классификации и определению направления движения цены (вверх или вниз) позволит нам:\n",
    "- Снизить шум: Упрощение задачи до бинарного прогноза делает модель более устойчивой к нерелевантным данным.\n",
    "- Повысить интерпретируемость: Модели классификации легче интерпретировать и использовать в торговых стратегиях.\n",
    "- Соответствовать рыночной практике: Трейдеры чаще интересуются направлением изменения, чем точной ценой.\n",
    "- \n",
    "Таким образом, классификация обеспечивает более достижимую и практически значимую цель для анализа финансовых новостей.\n",
    "\n",
    "В качестве эксперимента попробуем нелинейный метод - классификатор из XGBoost на основе метода градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3806c674-5a35-49d1-8e3a-18fbfd7f0e4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.53      0.51       994\n",
      "           1       0.49      0.46      0.48      1006\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.49      0.49      0.49      2000\n",
      "weighted avg       0.49      0.49      0.49      2000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[523 471]\n",
      " [545 461]]\n"
     ]
    }
   ],
   "source": [
    "# Stage 5: Pivot to binary classification\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "df = pd.read_parquet('sc454k_10k_summary_embeddings_sentiment_rs42.parquet')\n",
    "\n",
    "df['price_24h_change_percent'] = ((df['weighted_avg_24_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "# Create binary target: 1 for price increase, 0 for price decrease\n",
    "df['price_up'] = (df['price_24h_change_percent'] > 0).astype(int)\n",
    "\n",
    "# Drop rows with missing or infinite values in the target\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['price_up'])\n",
    "\n",
    "# Features: embeddings and sentiment probabilities\n",
    "X = df[[col for col in df.columns if col.startswith('embedding_')] + ['positive_prob', 'negative_prob']]\n",
    "y = df['price_up']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac348a-0b79-4e63-9391-5db7b2cbc4df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы**: результаты всё ещё неудовлетворительны, попробуем другие алгоритмы и другую целевую переменную - направление движения цены спустя не сутки, а час с момента выхода новости. Попробуем также исключить из признаков данные сентиментов. И, наконец, подберем наилучшие гиперпараметры с помощью GridSearch.\n",
    "\n",
    "Начнём с метода опорных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1d3b940f-c6d9-4042-938a-5686ddafa8db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters found:  {'C': 0.1, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'linear'}\n",
      "Accuracy: 0.50\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.48      0.58      1443\n",
      "           1       0.29      0.56      0.38       557\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.51      0.52      0.48      2000\n",
      "weighted avg       0.61      0.50      0.53      2000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[693 750]\n",
      " [247 310]]\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_parquet('sc454k_10k_summary_embeddings_sentiment_rs42.parquet')\n",
    "\n",
    "df['price_1h_change_percent'] = ((df['weighted_avg_1_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "# Create binary target: 1 for price increase, 0 for price decrease\n",
    "df['price_up'] = (df['price_1h_change_percent'] > 0).astype(int)\n",
    "\n",
    "# Drop rows with missing or infinite values in the target\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['price_up'])\n",
    "\n",
    "# Features: embeddings without sentiment probabilities\n",
    "X = df[[col for col in df.columns if col.startswith('embedding_')]]\n",
    "y = df['price_up']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Настройка гиперпараметров через GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    #'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['linear'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Инициализация SVC\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "# # Применяем GridSearchCV для подбора гиперпараметров\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие гиперпараметры\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Прогнозируем с лучшей моделью\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Подробный классификационный отчёт\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318268a-35cf-4b99-9331-91420ddb3de0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы:** \n",
    "- Точность не превышает случайное угадывание\n",
    "- Классы несбалансированы: класс 0 значительно преобладает над классом 1 (1443 против 557). Параметр class_weight='balanced' пытается компенсировать несбалансированность классов, но результатов всё ещё недостаточно.\n",
    "\n",
    "Модель испытывает сложности из-за несбалансированности классов, требуется использование техник балансировки. Сначала без балансировки попробуем более вычислительно легкий линейный алгоритм - Logistic Regression, с подбором параметров через GridSearch. Интервал в целевой переменной изменим на 12 часов с момента выхода новости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d98790da-c524-407c-8f15-ffa8fe007706",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best parameters found:  {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "Accuracy: 0.53\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.54      0.55      1087\n",
      "           1       0.48      0.51      0.49       913\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.53      0.53      0.53      2000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[587 500]\n",
      " [450 463]]\n"
     ]
    }
   ],
   "source": [
    "# LogReg\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "df = pd.read_parquet('sc454k_10k_summary_embeddings_sentiment_rs42.parquet')\n",
    "\n",
    "df['price_12h_change_percent'] = ((df['weighted_avg_12_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "# Create binary target: 1 for price increase, 0 for price decrease\n",
    "df['price_up'] = (df['price_12h_change_percent'] > 0).astype(int)\n",
    "\n",
    "# Drop rows with missing or infinite values in the target\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['price_up'])\n",
    "\n",
    "# Features: embeddings and sentiment probabilities\n",
    "X = df[[col for col in df.columns if col.startswith('embedding_')] + ['positive_prob', 'negative_prob']]\n",
    "y = df['price_up']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Настройка гиперпараметров для логистической регрессии\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear', penalty='l2', class_weight='balanced', max_iter=1000, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Подробный классификационный отчёт\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa16b91-65af-499b-8bad-918a4707d3d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы**: видим, что даже без балансировки результаты модели улучшились. Немного повысилась точность (теперь чуть лучше случайного угадывания), модель не имеет сильного прекоса в сторону одного класса, но не очень хорошо разделяет их. \n",
    "\n",
    "Попробуем сбалансировать данные и посмотреть на результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eb895a71-2366-4b9f-b1d5-9572c0feee09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best parameters found:  {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "Accuracy: 0.52\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54      1087\n",
      "           1       0.48      0.53      0.50       913\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.53      0.52      0.52      2000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[557 530]\n",
      " [426 487]]\n"
     ]
    }
   ],
   "source": [
    "# LogReg balanced\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df = pd.read_parquet('sc454k_10k_summary_embeddings_sentiment_rs42.parquet')\n",
    "\n",
    "df['price_12h_change_percent'] = ((df['weighted_avg_12_hrs'] - df['weighted_avg_0_hrs']) / df['weighted_avg_0_hrs'] * 100).round(2)\n",
    "\n",
    "df['price_up'] = (df['price_12h_change_percent'] > 0).astype(int)\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['price_up'])\n",
    "\n",
    "X = df[[col for col in df.columns if col.startswith('embedding_')] + ['positive_prob', 'negative_prob']]\n",
    "y = df['price_up']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Применяем undersampling для класса 0\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "y_train_df = pd.Series(y_train)\n",
    "\n",
    "# Сбрасываем индексы для согласования\n",
    "X_train_df.reset_index(drop=True, inplace=True)\n",
    "y_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Делим данные на два класса\n",
    "class_0 = X_train_df[y_train_df == 0]\n",
    "class_1 = X_train_df[y_train_df == 1]\n",
    "\n",
    "# Уменьшаем количество примеров класса 0\n",
    "class_0_downsampled = resample(class_0, \n",
    "                               replace=False,  # без повторов\n",
    "                               n_samples=len(class_1),  # количество примеров как в классе 1\n",
    "                               random_state=42)\n",
    "\n",
    "# Объединяем данные\n",
    "X_train_balanced = pd.concat([class_0_downsampled, class_1])\n",
    "y_train_balanced = pd.concat([pd.Series([0] * len(class_0_downsampled)), pd.Series([1] * len(class_1))])\n",
    "\n",
    "# Масштабируем сбалансированные данные\n",
    "X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear', penalty='l2', class_weight='balanced', max_iter=1000, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Подробный классификационный отчёт\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ef577-52bb-4d00-924d-ae70e78c882b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы**: результаты чуть ухудшились, но в целом остались на том же уровне, что и в предыдущем эксперименте.\n",
    "\n",
    "**Итоговые выводы**:\n",
    "В процессе работы были сделаны следующие наблюдения:\n",
    "- признаки сентиментов не влияют на качество моделей: если их убрать - метрики не менялись\n",
    "- количество признаков эмбеддингов не влияет на качество моделей: проверялось на 16, 64 и 768 - метрики практически не менялись\n",
    "- изменение временного интервала в таргете не влияет на качество моделей: проверялись 1, 2, 4, 8, 12, 24 часа - метрики практически не менялись\n",
    "\n",
    "Даже после балансировки классов точность модели остается невысокой. Это говорит о том, что линейные модели не очень хорошо подходят для данной задачи, и нужно работать в сторону усложнения используемых моделей, например, попробовать нелинейные. Также имеет смысл попробовать более сложные методы обработки данных - полиномиальные признаки и feature engineering. Еще одно направление для улучшения - увеличить количество данных для обучения, но нужно учитывать, что в контексте данной задачи получение суммаризации и последующие этапы обработки данных для получения эмбеддингов - это достаточно затратный по времени процесс. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}