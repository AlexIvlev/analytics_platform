{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance -q\n",
        "!pip install gensim -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH9fQr59lzY5",
        "outputId": "d147dea2-a7f1-46c4-ab62-bad020efce82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W3scPktwHfef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "import yfinance as yf\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                            precision_score,\n",
        "                            recall_score,\n",
        "                            f1_score)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "ENGLISH_STOP_WORDS = set( stopwords.words('english') ).union(set(ENGLISH_STOP_WORDS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB1qS2rIgKMn",
        "outputId": "dc696009-a490-4a3f-b1f1-b46a81660eb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJHx80evO9Zw",
        "outputId": "d0c6c88d-0da0-42d6-a98c-fb29a75abea0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_metrics(y_train, pred_train, y_test, pred_test):\n",
        "  print(f\"Accuracy_train: {accuracy_score(y_train, pred_train)}\")\n",
        "  print(f\"Precision_train: {precision_score(y_train, pred_train)}\")\n",
        "  print(f\"Recall_train: {recall_score(y_train, pred_train)}\")\n",
        "  print(f\"F1_train: {f1_score(y_train, pred_train)}\")\n",
        "  print(\"----------------------\")\n",
        "  print(f\"Accuracy_test: {accuracy_score(y_test, pred_test)}\")\n",
        "  print(f\"Precision_test: {precision_score(y_test, pred_test)}\")\n",
        "  print(f\"Recall_test: {recall_score(y_test, pred_test)}\")\n",
        "  print(f\"F1_test: {f1_score(y_test, pred_test)}\")"
      ],
      "metadata": {
        "id": "YD_8C2T8IfuH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best model"
      ],
      "metadata": {
        "id": "IO_JAUvcHwb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model is selected based on experiments conducted in https://github.com/AlexIvlev/analytics_platform/blob/main/notebooks/baseline/YP_2024_Reddit_baseline.ipynb\n"
      ],
      "metadata": {
        "id": "zqGocobtPgyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/drive/MyDrive/datasets/reddit_parser_2024_12_06_prices.parquet\")"
      ],
      "metadata": {
        "id": "mAfjuLF3IJ4o"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df['target'] = np.where(df['price_1d'] > df['created_price'], 1, 0)\n",
        "\n",
        "    drop_cols = ['id', 'title', 'url', 'created_utc', 'parsed_utc',\n",
        "                 'text', 'parent_id', 'clean_text', 'processed_text',\n",
        "                 'entities', 'tickers', 'price_1d', 'doc_embedding']\n",
        "\n",
        "    df = df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "    df.rename(columns={\"processed_text_length\": \"text_length\"}, errors='ignore', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess_data(df)"
      ],
      "metadata": {
        "id": "AyQrDM_mXU5I"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [col for col in df.columns if col != 'target']\n",
        "target_col = 'target'\n",
        "\n",
        "categorical_features = ['subreddit', 'type', 'ticker']\n",
        "numerical_features = [col for col in feature_cols if col not in categorical_features]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df[target_col], test_size=0.2, random_state=69)"
      ],
      "metadata": {
        "id": "e-1N0pvOY5dl"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = {\n",
        "    'n_estimators': 460,\n",
        "    'learning_rate': 0.10270500950873558,\n",
        "    'max_depth': 9,\n",
        "    'min_samples_split': 10,\n",
        "    'min_samples_leaf': 4,\n",
        "    'subsample': 0.9995540338327519,\n",
        "    'max_features': None\n",
        "}"
      ],
      "metadata": {
        "id": "Emb5nxMvZOVg"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('impute', SimpleImputer(strategy='constant', fill_value=0), ['num_comments']),\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "        ('num', 'passthrough', [col for col in numerical_features if col != 'num_comments'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(**best_parameters))\n",
        "])"
      ],
      "metadata": {
        "id": "8rshKJUiZHxy"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "pred_train = pipeline.predict(X_train)\n",
        "pred_test = pipeline.predict(X_test)\n",
        "\n",
        "classification_metrics(y_train, pred_train, y_test, pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDQinENwZjbg",
        "outputId": "d0b40d0e-6abb-4c29-8c3c-c8977c1d9bad"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_train: 0.9873737373737373\n",
            "Precision_train: 0.9885057471264368\n",
            "Recall_train: 0.9858220211161388\n",
            "F1_train: 0.9871620601117655\n",
            "----------------------\n",
            "Accuracy_test: 0.7713776722090261\n",
            "Precision_test: 0.7767332549941246\n",
            "Recall_test: 0.772196261682243\n",
            "F1_test: 0.7744581136496778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(pipeline, '/content/drive/MyDrive/datasets/reddit_gbc.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MeXadu9ucvC",
        "outputId": "81e36c4b-bf9e-448e-95dd-0d136db0b012"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/datasets/reddit_gbc.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference data pipeline"
      ],
      "metadata": {
        "id": "NAAydmpuPXAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Run https://github.com/AlexIvlev/analytics_platform/blob/main/parsers/reddit/reddit_parser.py to obtain the initial set of data"
      ],
      "metadata": {
        "id": "cUgOOihTRrOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/drive/MyDrive/datasets/reddit_parser_2025-03-10_23-18-05.parquet\")\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_YQ99GpPZQh",
        "outputId": "80213207-3109-47a3-82c8-8347b376acb4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16792, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "  # Remove rows with irrelevant content\n",
        "  df = df[~df['text'].isin(['', '[]', '[deleted]', '['])]\n",
        "\n",
        "  df['text_length'] = df['text'].apply(len)\n",
        "\n",
        "  # Sentiment Analysis\n",
        "  from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  df['sentiment_scores'] = df['text'].apply(lambda x: sia.polarity_scores(x).get('compound'))\n",
        "\n",
        "  df['clean_text'] = df['text'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word not in ENGLISH_STOP_WORDS]))\n",
        "\n",
        "  # Text Processing (Lemmatization + Stemming)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  def process_text(text):\n",
        "      doc = nlp(text)\n",
        "      processed_words = [stemmer.stem(token.lemma_) for token in doc if token.is_alpha]\n",
        "      return \" \".join(processed_words)\n",
        "\n",
        "  df['processed_text'] = df['clean_text'].apply(process_text)\n",
        "\n",
        "  df = df[~df['processed_text'].isin([''])]\n",
        "\n",
        "  # Named Entity Recognition (NER)\n",
        "  def get_entities(text):\n",
        "      doc = nlp(text)\n",
        "      return list(set(ent.text for ent in doc.ents if ent.label_ == 'ORG'))\n",
        "\n",
        "  df['entities'] = df['text'].apply(get_entities)\n",
        "  df = df[df['entities'].apply(lambda x: len(x) != 0)]\n",
        "\n",
        "  # SEC Ticker List Filtering\n",
        "  url = \"https://www.sec.gov/files/company_tickers.json\"\n",
        "  headers = {\"User-Agent\": \"john doe johndoe@gmail.com\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  response.raise_for_status()\n",
        "  sec_data = response.json()\n",
        "\n",
        "  sec_tickers = {entry['ticker'].upper() for entry in sec_data.values()}\n",
        "  sec_tickers.add('^GSPC')\n",
        "\n",
        "  def process_entities(entities):\n",
        "      return [e.upper() if e != \"S&P\" else \"^GSPC\" for e in entities if e.upper() in sec_tickers]\n",
        "\n",
        "  df['tickers'] = df['entities'].apply(process_entities)\n",
        "  df = df[df['tickers'].apply(lambda x: len(x) != 0)]\n",
        "\n",
        "  # Select random ticker from tickers as the 'main' ticker\n",
        "  df['ticker'] = df['tickers'].apply(lambda x: random.choice(x))\n",
        "\n",
        "\n",
        "  # Doc2Vec Embeddings\n",
        "  df['id'] = df['id'].astype(str)\n",
        "\n",
        "  tagged_data = [TaggedDocument(words=text.split(), tags=[doc_id]) for text, doc_id in zip(df['processed_text'], df['id'])]\n",
        "\n",
        "  model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=40)\n",
        "  model.build_vocab(tagged_data)\n",
        "  model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "  df['doc_embedding'] = df['id'].map(lambda x: model.dv[x])\n",
        "\n",
        "  return df\n",
        "\n",
        "df = preprocess_data(df)"
      ],
      "metadata": {
        "id": "pcdfxSPFgZ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add prices (do it at least 2 days after parsing)"
      ],
      "metadata": {
        "id": "15A5rilxlf1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['created_utc'] = pd.to_datetime(df['created_utc'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df['created_price'] = np.nan\n",
        "df['price_1d'] = np.nan"
      ],
      "metadata": {
        "id": "oj_NJevUlkyW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ticker_price(ticker, target_time, price_type='Close', interval='30m', sleep_seconds=0):\n",
        "  date = target_time.date()\n",
        "  prices = yf.download(ticker, start=str(date), end=str(date + pd.Timedelta(days=1)), interval=interval, progress=False)\n",
        "\n",
        "  if prices.empty:\n",
        "    return 0\n",
        "\n",
        "  prices.index = prices.index.tz_localize(None)\n",
        "\n",
        "  prices['time_diff'] = abs(prices.index - target_time)\n",
        "  closest_time = prices['time_diff'].idxmin()\n",
        "  closest_row = prices.loc[closest_time]\n",
        "\n",
        "  time.sleep(sleep_seconds)\n",
        "\n",
        "  try:\n",
        "    return closest_row[price_type, ticker]\n",
        "  except KeyError:\n",
        "    print(\"KeyError occurred, returning 0\")\n",
        "    return 0"
      ],
      "metadata": {
        "id": "8P9fzAhvmKTw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_prices(df, price_col, date_col, plus_days=0):\n",
        "\n",
        "  start_index = 0\n",
        "\n",
        "  for i in range(start_index, df.shape[0], 100):\n",
        "      print(i)\n",
        "      df_tick_slice = df.iloc[i:i+100].copy()\n",
        "\n",
        "      df_tick_slice[price_col] = df_tick_slice.apply(lambda x: get_ticker_price(x['ticker'], x[date_col] + pd.Timedelta(days=plus_days)) if pd.isnull(x[price_col]) else x[price_col], axis=1)\n",
        "\n",
        "      df.loc[df_tick_slice.index, price_col] = df_tick_slice[price_col]\n",
        "\n",
        "      # Save to parquet after each batch\n",
        "      df.to_parquet('/content/drive/MyDrive/datasets/reddit_parser_2025_03_10_processed.parquet', index=False)\n",
        ""
      ],
      "metadata": {
        "id": "HGGr3mFGm2SN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_prices(df, 'created_price', 'created_utc', 0)\n",
        "df = df[df['created_price'].notnull() & (df['created_price'] != 0)]\n",
        "add_prices(df, 'price_1d', 'created_utc', 1)\n",
        "df = df[df['price_1d'].notnull() & (df['price_1d'] != 0)]"
      ],
      "metadata": {
        "id": "efUYHy1HmT9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_inference = df.copy()"
      ],
      "metadata": {
        "id": "WDZaNblJuTJA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_2nd_stage.to_parquet('/content/drive/MyDrive/datasets/reddit_parser_2024_12_06_prices.parquet', index=False)"
      ],
      "metadata": {
        "id": "vYjneyP3oDP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "3MvHTPaDwoFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_inference['target'] = np.where(df_inference['price_1d'] > df_inference['created_price'], 1, 0)"
      ],
      "metadata": {
        "id": "HQDGcHBswmxZ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df['target'] = np.where(df['price_1d'] > df['created_price'], 1, 0)\n",
        "\n",
        "    drop_cols = ['id', 'title', 'url', 'created_utc', 'parsed_utc',\n",
        "                 'text', 'parent_id', 'clean_text', 'processed_text',\n",
        "                 'entities', 'tickers', 'price_1d', 'doc_embedding']\n",
        "\n",
        "    df = df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess_data(df_inference)"
      ],
      "metadata": {
        "id": "3GPkWTXnxeKL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [col for col in df.columns if col != 'target']\n",
        "target_col = 'target'\n",
        "\n",
        "X_inf, y_inf = df[feature_cols], df[target_col]"
      ],
      "metadata": {
        "id": "oANGlYQLyMw-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = joblib.load('/content/drive/MyDrive/datasets/reddit_gbc.pkl')"
      ],
      "metadata": {
        "id": "uug_m525yaXZ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_inference = pipeline.predict(X_inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5mNGu7Iw9_g",
        "outputId": "ea04c827-ce6e-48d0-8e7a-ab045b896b15"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  print(f\"Accuracy_inference: {accuracy_score(y_inf, pred_inference)}\")\n",
        "  print(f\"F1_inference: {f1_score(y_inf, pred_inference)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBmr_rShzZNK",
        "outputId": "65d27519-49e2-4e96-e128-c0d089a252b8"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_inference: 0.4121510673234811\n",
            "F1_inference: 0.4371069182389937\n"
          ]
        }
      ]
    }
  ]
}