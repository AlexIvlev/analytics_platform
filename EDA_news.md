# Результаты EDA

## Общий анализ

Проведенный EDA показал высокое качество датасета **SC454k**. Все тексты содержат релевантные финансовые данные и **не требуют значительной предобработки**, за исключением **фильтрации аномалий**, таких как слишком длинные или короткие тексты.

Всего в датасете **453 932 образцов**. Этого достаточно и для целей обучения моделей ML, и для целей DL.

**Анализ данных на пропуски показал**, что в ключевых для задач проекта признаках **Article** и **weighted_avg_x_hrs** пропусков не имеется. Имеется незначительное в рамках датасета количество нулевых значений у признаков **Sector** и **Industry**, но в случае экспериментов с разбиением датасета на сектора и отрасли эти образцы могут быть проигнорированы без ущерба для дальнейших исследований.

После очистки текстов Article от выбросов (5-й и 95-й процентили, оставлены центральные 90% значений), **средняя длина текста новости составила 755 слов, или 6004 символов**. Для моделей transformers, которые будут использоваться в дальнейшем для работы, такая длина может превышать максимально допустимое количество токенов, поэтому **потребуется предварительная суммаризация** текстов.

Всего в датасете прдставлены **тексты по 1393 компаниям**. Большая часть датасета - **новости**, но имеется также и значительная доля **пресс-релизов**.

**Анализ сезонности** показал следующие закономерности: в выходные дни количество новостей резко снижается, и пресс-релизы отсутствуют. В рабочие дни наблюдается постепенный рост количества новостей, достигающий пика к пятнице. По месяцам года **сезонность выражена слабо**, за исключением резкого всплеска в апреле, связанного с аномальным увеличением новостей во втором квартале 2020 года. Возможные причины могут включать экономические условия на фоне пандемии COVID и особенности сбора данных.

В разбивке по секторам и отраслям экономики **сезонных колебаний в количестве новостей не наблюдается**: доля новостей для разных секторов и отраслей остается относительно стабильной (за исключением вышеупомянутого всплеска в апреле).

## Целевые переменные

**Цель проекта** - предсказать изменение цены на основе текстов новостей, что представляет собой **задачу регрессии**. Если в рамках такой постановки не удастся получить значимые результаты, задача будет изменена на **классификацию**, с целью предсказания диапазона изменения цены.

Преимущество датасета в том, что он включает данные по **small-cap компаниям** (компаниям малой капитализации), чьи акции характеризуются повышенной волатильностью и более выраженной реакцией на новостной фон. Это должно позволить более точно выявить связь между текстами новостей и изменениями цен, и точнее спрогнозировать движения акций таких компаний.
 
**Целевой переменной** был выбран **процент изменения цены через x часов с момента публикации новости**. Были добавлены метки для следующих интервалов: 
- 1h
- 2h
- 4h
- 8h
- 12h
- 24h
- 48h
- 72h
- 96h
- 360h
- 720h

 После построения корреляционной матрицы **наиболее значительные корелляции** были вявлены между следующими парами признаков:
 - 1h и 2h
 - 4h и 8h
 - 4h и 12h
 - 8h и 12h
 - 8h и 48h
 - 12h и 48h
 - 360h и 720h
 
 Корреляция между ближайшими интервалами говорит о том, что можно использовать любой из них в качестве целевой переменной. Корреляция между отдаленными интервалами показывает, как долго сохраняется импульс движения цены, и позволяет делать обоснованные предположения о его соханенении на определенный период в будущем. В этом плане наиболее интересными представляются корреляции между **8h и 48h**, а также между **360h и 720h**. Проведенный анализ корреляций помогает выбрать следующие интрвалы в качестве целевых переменных:
 - 1h (краткосрочный, быстрое влияние на цену)
 - 8h (среднесрочный, влияние на цену в рамках одной торговой сессии)
 - 360 (долгосрочный, влияние на цену в течение значительного периода времени)


## Feature engineering

После анализа датасета было принято решениние дополнить его следующими признаками на этапе машинного обучения:
- **summary**: суммаризация исходного текста, будет использоваться для получения следующих признаков
- **sentiment**: тональность по summary, с вероятностями
- **embedding**: эмбеддинг по summary

Признаки будут получены с помощью моделей-трансформеров из библиотеки [Transformers](https://github.com/huggingface/transformers) от [Hugging Face](https://huggingface.co/docs/transformers/main/en/index).

**Плюсы такого подхода**:
- библиотека предоставляет доступ через API ко всем необходимым инструментам обработки текста (например, токенизаторы) и большому количеству предобученных моделей, среди которых есть модели, тонко настроенные на финансовую тематику
- архитектура трансформеров учитывает контекст и может обрабатывать разнообразные формы и последовательности слов, что позволяет получать качественный результат и существенно экономит время на предобработке текстов

**Проблемы**: 
- высокие вычислительные затраты. Путем экспериментов было установлено: получение summary с помощью модели [bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) для 10 000 новостей в бесплатном Google Colab с графическим процессором T4 занимает около 4 часов
- ограничения на длину текста, что затруднит обработку текстов без их предварительного сокращения, например, с помощью суммаризации

В связи с высокими требованиями к ресурсам для обработки всего объема текстовых данных были приняты следующие **решения по использованию датасета**:
- обучать линейные модели на фиксированной части датасета размером от 10 000 до 50 000 записей, в зависимости от доступных ресурсов
- обучать нелинейные модели на части от 50 000 до 100 000 записей
- использовать полный датасет только на этапе глубинного обучения, например, в задачах дообучения или переноса обучения
